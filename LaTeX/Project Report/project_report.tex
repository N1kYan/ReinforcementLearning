%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Initialize:}}

\usepackage{verbatim} %TODO delete before submission
\usepackage[numbers]{natbib}%TODO delete before submission

%TODO Zitierweise Ã¤ndern, sieht schrecklich aus
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Application of Reinforcement Learning Methods
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
\subtitle{Group 19 - Final Project Report}

%\titlerunning{Short form of title}        % if too long for running head

\author{Yannik Frisch \and Tabea Wilke \and Maximilian Gehrke %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{F. Author \at
              first address \\
              Tel.: +123-45-678910\\
              Fax: +123-45-678910\\
              \email{fauthor@example.com}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           S. Author \at
              second address
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
TODO
\keywords{DDPG \and NAC \and BallBalancer \and Pendulum \and DoublePendulum \and FurutaPendulum}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{sec:intro}
We present our final results of evaluating the reinforcement learning algorithms \textit{Deep Deterministic Policy Gradient} and \textit{Natural Actor Critic} on the simulated quanser systems platforms \textit{Pendulum-v1}, \textit{BallBalancerSim-v1}, \textit{Qube-v0}, \textit{DoublePendulum-v0}. We also present the results of the NAC algorithm on the real double pendulum hardware system.
\newpage
\section{Deep Deterministic Policy Gradient}
\label{sec:ddpg}
The \textit{Deep Deterministic Policy Gradient} approach [x] is an application of the \textit{Deep Q-Learning} algorithm [x] to actor-critic methods [x] in combination with the \textit{Deterministic Policy Gradient} [x]. 
\subsection{Evaluation on Pendulum-v1}
The OpenAI gym environment \textit{Pendulum-v1} consists of of a singular pendulum attached on one side. The reward is depending on the angle, with a max reward of [?] per time-step for balancing the pendulum straight upright. DDPG was able to learn a good policy for this environment in less than 1000 episodes.[EDIT: PARAMETERS]\\

\subsection{Evaluation on BallBalancerSim-v0}
The Quanser Robots \textit{BallBalancerSim-v0} environment consist of a plate whose angles can be controlled by the input actions. The goal is to balance a ball on the plate, receiving a maximum reward of [?] per time-step for balancing it in the middle of the plate. The environment ends after a maximum of 1000 time-steps.\\
We started our evaluations with using the same network structures for the actor and critic as [x] did. We used 2 hidden layers with 100 and 300 hidden neurons for the actor and the critic networks and their targets. The learning rates are also set to $\alpha_{actor}=1e-4$ and $\alpha_{critic}=1e-3$. 
In figure [x] one can find our first acceptable results. The discounting is set to $\gamma=0.99$, we did small target updates ($\tau=1e-4$), used a mini-batch size of 64 and a total replay buffer size of 1e6. We slightly increased the noise to $\sigma_{OU}=0.2$ and $\theta_{OU}=0.25$ as the environments action space has an higher amplitude compared to the \textit{Pendulum-v0}. 
\begin{figure}[H]
	\includegraphics[width=0.5\textwidth]{plots/ddpg_ball_first_train.png}
	\includegraphics[width=0.5\textwidth]{plots/ddpg_ball_first_eval.png}
	\caption{The left figure shows the cumulative reward per episode during the training process. The right one displays current reward per time-step for 25 episodes of evaluation [EDIT: DO 100 EPISODES!]}
	\label{ddpg:ball:first}
\end{figure}
The algorithm did learn to balance the ball, but was not very stable, which can also be read from the learning process plot. To further increase the stability, we increased th mini-batch size used to sample from the replay buffer, and reduced the noise again. Using weight regularization did not seem to be helpful, so we set it to zero.\\
Figure [x] shows the training process where discounting is set to $\gamma=0.2$ compared to $\gamma=0.99$. One can see discounting is crucial to solve this environment.\\
\begin{figure}[H]
	\includegraphics[width=0.5\textwidth]{plots/ddpg_ball_low_gamma.png}
	\includegraphics[width=0.5\textwidth]{plots/ddpg_ball_high_gamma.png}
	\caption{The left figure shows the cumulative reward per episode during the training process with $\gamma$ set to 0.2. The right one displays the process for $\gamma=0.99$. Using discounting close to 1 was very important.}
	\label{ddpg:ball:gamma}
\end{figure}
We tried to reduce the computational effort by only using a single hidden layer with 100 hidden neurons instead of two layers. The impact on the performance is shown in figure [x]. The learning suffered from instabilities, so we decided to weight the stability of using two hidden layers higher than the performance loss.
\begin{figure}[H]
	\includegraphics[width=0.5\textwidth]{plots/ddpg_ball_1layer_train.png}
	\includegraphics[width=0.5\textwidth]{plots/ddpg_ball_1layer_eval.png}
	\caption{The left figure shows the cumulative reward per episode during the training process using only a hidden layer and the right one again displays the performance during evaluation.}
	\label{ddpg:ball:gamma}
\end{figure}
Our best results can be found in figure \ref{ddpg:ball:best} where we set the OU action noise equal to the one used in the original paper with $\sigma_{OU}=0.15$ and $\theta_{OU}=0.2$. We used slightly harder updates with $\tau=1e-3$, and achieved an average cumulative reward of about 650 for 25 episodes of evaluation. The learning process took about 3 hours for 1000 episodes.
\begin{figure}[H]
	\includegraphics[width=0.5\textwidth]{plots/ddpg_ball_best_train.png}
	\includegraphics[width=0.5\textwidth]{plots/ddpg_ball_best_eval.png}
	\caption{The left figure shows the cumulative reward per episode during the training process. The right one displays the current reward per time-step for every evaluation episode.}
	\label{ddpg:ball:best}
\end{figure}
\subsection{Evaluation of the Pretrained Model on the Real Ball Balancer System}

\subsection{Evaluation on Qube-v0}
The Quanser Robots \textit{Qube-v0} environment implements the Furuta Pendulum, which consists of a motor controlling one horizontal arm. One end of the joint is attached to the motor, the other end is attached to another vertical arm, which can only be controlled indirectly by controlling the first arm. See [x] for more details.\\
The goal is to balance the second arm in upright position, receiving a maximum reward of 0.02 per time-step. The environment stops after 300 time-steps.\\
We did not get any useful results re-using the parameters we found for \textit{BallBalancer-v0} 
\section{Natural Actor Critic}
\label{sec:nac}
\subsection{Evaluation on CartPole-v0}
\subsection{Evaluation on the BallBalancerSim-v0}
\subsection{Evaluation of the Pretrained Model on the Real Ball Balancer System}
\subsubsection{Learning from the Physical System}

\section{Discussion}
\label{sec:conclusion}






\begin{comment}

We propose several possible extensions and show their performance on a task.\\
Text with citations \cite{RefB} and \cite{RefJ}.
\subsection{Subsection title}
\label{sec:2}
as required. Don't forget to give each section
and subsection a unique label (see Sect.~\ref{sec:1}).
\paragraph{Paragraph headings} Use paragraph headings as needed.
\begin{equation}
a^2+b^2=c^2
\end{equation}

% For one-column wide figures use
\begin{figure}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\includegraphics{example.eps}
% figure caption is below the figure
\caption{Please write your figure caption here}
\label{fig:1}       % Give a unique label
\end{figure}
%
% For two-column wide figures use
\begin{figure*}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\includegraphics[width=0.75\textwidth]{example.eps}
% figure caption is below the figure
\caption{Please write your figure caption here}
\label{fig:2}       % Give a unique label
\end{figure*}


%
% For tables use
\begin{table}
% table caption is above the table
\caption{Please write your table caption here}
\label{tab:1}       % Give a unique label
% For LaTeX tables use
\begin{tabular}{lll}
\hline\noalign{\smallskip}
first & second & third  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
number & number & number \\
number & number & number \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}


\end{comment}
%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{ddpg.bib}   % name your BibTeX data base

% Non-BibTeX users please use
\begin{comment}


\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
\bibitem{RefJ}
% Format for Journal Reference
Author, Article title, Journal, Volume, page numbers (year)
% Format for books
\bibitem{RefB}
Author, Book title, page numbers. Publisher, place (year)
% etc

\end{thebibliography}
\end{comment}
\end{document}
% end of file template.tex
