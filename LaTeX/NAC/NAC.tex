%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}

%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}

\usepackage{natbib} % delete before submission
\bibliographystyle{plain} % delete before submission
\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage[noend]{algpseudocode}

%\usepackage{hyperref}

\newcommand{\x}{\item}
\newcommand{\parTitle}[1]{\textbf{#1:}}
\DeclareMathOperator{\E}{\mathbb{E}}

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}
\title{Natural Actor-Critic: Components and Extensions
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Maximilian A. Gehrke\and\\Yannik P. Frisch\and Tabea A. Wilke
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Maximilian A. Gehrke \at
              \email{maximilian\_alexander.gehrke@stud.tu-darmstadt.de}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           Yannik P. Frisch \at
           \email{yannik\_phil.frisch@stud.tu-darmstadt.de}
           \and
           Tabea A. Wilke \at
           \email{tabeaalina.wilke@stud.tu-darmstadt.de}
}
\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor
\maketitle

 % ------------------------------ TASKS -------------------------------------- %
 
% (1) Formality & Language: The report uses the LaTex template and does not exceed 8 pages + 2 pages references. The report is understandable and does not contain any typos, slang or any other errors [10 points]
% (2) Structure & Figures: The report is well structured and has a coherent story that integrates the individual papers known from literature into a bigger picture. Figures and diagrams are well described, labeled, and informative. [10 points]
% (3) Overview: The report provides an extensive overview about the existing literature and provides a summary of the algorithm/platform, variations thereof and its applications [15 points]
% (4) Discussion & Contribution: Besides summarizing, the report compares the existing literature and highlights the differences between approaches. Thereby, introducing a new perspective / overview, which is not present in current literature.

 % ------------------------------- Abstract ---------------------------------- %
\begin{abstract}
In this paper we describe the natural actor-critic approach and provide an extensive overview of the current research. This includes a basic description of the natural gradient, an introduction to actor-critic approaches and comparisons between existing natural actor-critic modifications. 


\keywords{Natural Gradient \and Advantage Function \and Actor-Critic \and NAC}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

% ---------------------- Introduction --------------------------------- %
\section{Introduction}
\label{sec:intro}

Natural actor-critic (NAC) methods \citep{peters2005natural} have been very successful in the last two decades. They could be applied to various fields, including traffic optimization \citep{richter2007natural}, dialog systems \citep{jurvcivcek2011natural} and high dimensional control tasks \citep{park2005rls, peters2008natural, peters2003reinforcement}. The NAC is an actor-critic policy gradient method (PGM), which optimizes a policy using gradient ascent.

PGMs represent a policy by using differentiable function approximation. They optimize a scalar performance measure $J$, called objective function, by repeatedly estimating its gradient w.r.t. the policy parameters and updating the policy parameters a proportion in its direction. PGMs have several advantages. In comparison to value-based algorithms, policy gradient methods have very good sample complexity guarantees \cite{nemirovski2005efficient} and convergence properties. In addition, PGMs are model-free, can learn stochastic policies and are effective in high-dimensional or continuous action spaces. Further, with PGM we can introduce a prior on the policy, we can converge to a deterministic policy and do not have to choose a suboptimal action for exploration purposes and we can choose our actions stochastically. However, PGMs are typically inefficient, have high variance and typically converge to a local rather than a global optimum. It is also necessary, that the policy is differentiable w.r.t its parameters, which is not always the case.

Actor-critic methods are special cases of PGMs. They approximate a value function (typically a state-action value function), which helps to approximate the policy. This means that we have to estimate two sets of parameters: the parameters of the policy (\textit{actor}) and the parameters of the value function (\textit{critic}). The actor tells the agent, which actions to execute. The agent executes the action in the environment and the environment returns an observation. The critic rates the observations and updates its own parameters. Immediately afterwards or if a specific criterion is met, the actor updates its parameters w.r.t the critic.

The NAC utilizes the natural gradient in an actor-critic environment to optimize a policy. How this works and possible modifications to the NAC will be the scope of this paper. We start by setting up some preliminaries in section \ref{sec:preliminaries}, before we introduce and discuss the natural gradient in section \ref{sec:NG}. The natural actor-critic algorithm is presented in section \ref{sec:nac} and modifications to the NAC in section \ref{sec:modifications}. Finally, we close with a discussion in section \ref{sec:discussion}.


% ----- PRELIMINARIES ----- %
\section{Preliminaries}
\label{sec:preliminaries}

We consider a standard reinforcement learning framework, in which a learning agent interacts with a Markov Decision Process (MDP) \citep{howard1960dynamic, sutton2018reinforcement}. For each discrete time step $t \in \{0,1,2,...\}$, the state, action and reward is denoted as $s_t \in \mathit{S}$, $a_t \in A$ and $r_{t+1} \in \mathit{R} \subset \mathbb{R}$ respectively. The dynamics of the environment are described by the state-transition probabilities $p(s|s', a) = \text{Pr}\{S_t = s' | S_{t-1} = s, A_{t-1} = a\}$ and the expected immediate rewards $r(s, a) = \E[R_t | S_{t-1} = s, A_{t-1} = a]$, for all $s, s' \in S, a \in A$. The agent's behavior at each time step $t$ is specified by a policy $\pi_{\theta}(a|s) = \text{Pr}\{A_t = a | S_{t} = s, \theta\}$, where $\theta$ denotes the parameters of the policy. 
\\\\
We assume that $\pi$ is differentiable w.r.t. its parameters, so that $\tfrac{\partial\pi(a|s)}{\partial \theta}$ exists and we can estimate the gradient of the objective function $J(\theta)$ by applying the policy gradient theorem \citep{sutton2000policy}

\begin{equation}
\label{equ:policygradienttheorem}
\nabla_{\theta}J(\theta)=\E_{\pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s) Q^{\pi_{\theta}}(s, a)],
\end{equation}

\noindent where $Q^{\pi_{\theta}}(s, a)$ denotes an action-value function. One of the most basic policy gradient algorithms, \textit{REINFORCE} \citep{williams1992simple}, estimates the action-value function $Q^{\pi_{\theta}}$ by using the expected discounted return (also known as Monte-Carlo return), \(Q^{\pi_\theta}(s, a) \approx \mathit{G}_t = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\),
where $\gamma \in [0, 1]$ denotes a discount factor. With the gradient of the objective function $\nabla_{\theta}J(\theta)$ and a learning rate $\alpha \in \left[0,1\right]$, policy gradient methods recursively update the parameters of the policy, $\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} J(\theta)$. After a finite number of steps, the parameters converge to a local optimum. 

Instead of using a Monte-Carlo estimate directly, actor-critic methods employ the critic to model the action-value function with a function approximation $Q^{\pi_\theta}(s, a) \approx Q_w(s,a)$ \citep{sutton2000policy}, where $w$ denotes the parameters of the critic, which need to be optimized. In addition, the introduction of a baseline $B(s,a)$, reduces the variance of the action-value function estimate and accelerates learning \citep{sutton2018reinforcement}: \(
\nabla_{\theta}J(\theta)=\E_{\pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s) Q_w(s, a) - B(s,a)]\). A good baseline with minimal variance is the value function. Subtracting the value function from the action-value function yields the advantage function $A(s,a) = Q(s,a)-V(s)$. The critic is able to directly estimate the advantage function $A_w(s,a)$:

\begin{equation}
	\label{eq:3}
	\nabla_{\theta}J(\theta)=\E_{\pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s) A_w(s, a)].
\end{equation}

\noindent Equation \ref{eq:3} will help us understand how the NAC algorithm in section \ref{sec:nac} works. Beforehand, we need to get an understanding of the natural gradient.

% ------ NATURAL GRADIENT ----- %
\section{Natural Gradient}
\label{sec:NG}

The natural gradient was first introduced by Amari in 1998 \cite{amari1998natural}. The difference between the natural gradient and the ordinary vanilla gradient is the direction it points to. The ordinary vanilla gradient only points to the steepest direction, if the parameter space is orthonormal and has a Euclidean character. The natural gradient, however, points to the steepest direction of a Riemann parameter space (e.g. neural networks  \citep{amari1998natural}).

A Riemann parameter space is a differentiable manifold, where an inner product $<\cdot , \cdot >$ exists for each tangent space. For two tangent vectors $\mathbf{u}$ and $\mathbf{v}$, the inner product $<\mathbf{u}, \mathbf{v}>$ yields a real number. This makes it possible to define notions such as length, areas, angles or volumes. To calculate the gradient, we need to be able to calculate the squared length of a small incremental vector $d\textbf{w}$ connecting a point $\textbf{w}$ and $\textbf{w} + d\textbf{w}$. Equation \ref{eq:riem} shows the formula for Riemann spaces (on the left) and Euclidean spaces (on the right):

\begin{equation}
	\label{eq:riem}
	|d\textbf{w}|^2 = \sum_{i,j}g_{ij}(\textbf{w}) dw_i dw_j ~,~ |d\textbf{w}|^2 = \sum_{i=1}^{n}(dw_i)^2,
\end{equation}

\noindent where $g_{ij}(\textbf{w})$ is a function, enabling us to create a measure of distance. It can be written as an $n \times n$ matrix $G = (g_{ij})$, called Riemannian metric tensor, and reduces to the unit matrix $I$ in the case of a Euclidean orthonormal parameter space. Therefore, the Riemannian distance measure is a generalization of the Euclidean orthonormal measure of distance \citep{haykin2009neural, amari1998natural}. We can utilize the Riemannian metric tensor to construct a gradient which points in the steepest direction of a Riemannian space:

\begin{equation}
	\label{eq:ngradient}
	\widetilde{\nabla}_{\theta} J(\theta) = G^{-1} \nabla_\theta J(\theta).
\end{equation}

\noindent $\widetilde{\nabla}_{\theta}$ is the natural gradient w.r.t the parameters $\theta$.  Learning should be carried out with a gradient descent like update rule: $\theta_{t+1} = \theta_{t} + \alpha \widetilde{\nabla}_{\theta} J(\theta)$, where $\alpha$ denotes a learning rate as usual. In the special case that the parameter space is Euclidean and the coordinate system orthonormal, the conventional gradient equals the natural gradient: $\widetilde{\nabla}_{\theta} J(\theta) = \nabla_{\theta}$.
If the Fisher information matrix (FIM) exists, it has been shown that we can use it as the Riemannian metric tensor in equation \ref{eq:ngradient} to get the natural gradient \citep{peters2008natural, amari1998efficiently}. The FIM of a policy is defined as:

\begin{equation}
	F_\theta=\E_{s,a}\left[\nabla_{\theta}\log\pi_{\theta}(a|s)^{T}\nabla_{\theta}\log\pi_{\theta}(a|s)\right].
\end{equation}

\noindent If we look at the problem from a different angle, we can see why this approach is unique. Policy gradient methods change the parameters of the policy in order to maximize an objective function $J(\theta)$. This can be done by taking the vanilla gradient (equation \ref{ng1}). However, the vanilla gradient has the downfall that in flat regions of the parameter space, the algorithm moves very slowly, whereas in steep regions the algorithm moves very fast and may even shoot beyond the local optimum. The reason for this is that for every parametrization of $\pi_{\theta}$ the gradient is different. This is why we introduce the Kullback-Leibler divergence, a measure of distance between two distributions, which can be approximated by the second-order Taylor expansion (equation \ref{ng2}). We constrain the Kullback-Leibler divergence to be less than a fixed value $\epsilon$, so that for each update the parameters of $\pi_\theta$ change exactly for a given distance in parameter space. Equation \ref{ng1} and \ref{ng2} together form an optimization problem:

\begin{align}
	\max_{\delta\theta} J(\theta + \delta\theta) \approx J(\theta) + \delta\theta^T\nabla_\theta J(\theta)\label{ng1}\\
	\text{s.t. } \epsilon = D_{KL}(\pi_{\theta} || \pi_{\theta + \delta\theta}) \approx \tfrac{1}{2} \delta\theta^T F_\theta \delta\theta\label{ng2}.
\end{align}

\noindent The solution of the optimization problem yields equation \ref{eq:ngradient}, applied with the FIM. Now we can see why the natural gradient is so unique: it is invariant to parametrization \citep{pascanu2013revisiting, peters2008natural}. In addition, the natural gradient (NG) has some other important properties, which we would like to mention:

\begin{itemize}
	\x \parTitle{Online Learning} The NG can be used online and therefore can learn from incomplete sequences and reduce the variance of the action-value function estimation \cite{pascanu2013revisiting, peters2008natural}.
	\x \parTitle{1st order method} The NG is a first order method, but implements second order advantages \cite{pascanu2013revisiting}. This is especially relevant for problems, where the cost function is only accessible indirectly \cite{desjardins2013metric}.
	\x \parTitle{Better \& faster convergence} In many cases, the NG converges faster than vanilla gradient methods and avoids getting stuck in plateaus \cite{amari1998efficiently, sohl2012natural}.
	\x \parTitle{Drawbacks} The Riemannian metric tensor needs to be nonsingular and invertible. This is not always the case and even if it is the case, the inversion of a matrix is very costly. In addition, by applying the NG, the policy variance might reduce to zero, which in turn reduces the exploration to zero. However, exploration is needed to find an optimal policy.
\end{itemize}

% ---------------------- NAC algorithm --------------------------------- %
\section{Natural Actor-Critic}
\label{sec:nac}
In this paper, we focus on the trajectory-based formulation of the \textit{Natural Actor-Critic} (NAC) algorithm \citep{peters2005natural}, called episodic NAC (eNAC, algorithm \ref{enac-algo}).

\begin{algorithm}
	\caption{Episodic Natural Actor-Critic (eNAC)}\label{enac-algo}
	\begin{algorithmic}[1]
		\REQUIRE Parameterized policy $\pi_{\theta}(a|s)$ and its derivative $\nabla_\theta\log\pi_{\theta}(a|s)$\\
		\hspace{1.05cm}  with initial parameters $\theta=\theta_0$.
		\FOR{$u = 1,2,3,\dots$}
		\FOR{$e = 1,2,3,\dots$}
		\STATE \textbf{Execute roll-out:} Draw initial state $s_0 \sim p(s_0)$
		\FOR{$t =1,2,3,\dots,N$}
		\STATE Draw action $a_t\sim\pi_{\theta_t}(a_t|s_t)$, observe next state $s_{t+1} \sim p(s_{t+1}|s_t, a_t)$\\
		and reward $r_{t+1} = r(s_t, a_t)$.
		\ENDFOR
		\ENDFOR
		\STATE \textbf{Critic Evaluation (repeat for each sampled trajectory):} Determine compatible function approximation of advantage function $A(s,a) \approx A_{w_t}(s, a)$.
		\STATE Determine basis functions: $\Phi_e = \left[\sum_{t=0}^T\gamma^t\nabla_\theta\log\pi_{\theta}(a_t|s_t)^T, 1\right]^T$, \\
		reward statistics: $R_e=\sum_{t=0}^T\gamma^t r_t$ and solve $\begin{bmatrix} w_{e}\\J \end{bmatrix} = (\Phi_e^T \Phi_e)^{-1} \Phi_e^T R_e$.\\
		Update critic parameters: $w_{t+1} = w_t + \beta w_{e}$.
		
		\STATE \textbf{Actor Update:} When the natural gradient is converged, $\measuredangle (w_{t+1}, w_t)\leq\epsilon$, update the policy parameters: $\theta_{t+1} = \theta_t + \alpha w_{t+1}$.
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\noindent The eNAC algorithm has a fixed amount of updates $u$ and a fixed amount of steps the agent executes every update. Therefore, if a trajectory $e$ has reached a terminal state before the agent executed all its steps, the algorithm samples a new trajectory. This repeats until the maximum number of steps is met and the current trajectory is interrupted. During this process, all states, actions and rewards are stored for each trajectory.

Afterwards, we estimate $w_e$ by determining compatible function approximations, basis functions and reward statistics for the samples of a single trajectory $e$ and solve a linear system of equation. $w_t$ is then updated by adding a proportion of $w_e$. We repeat this process for all trajectories encountered during the update. If the angle between $w_{t+1}$ and $w$ is smaller than some fixed value $\epsilon$, we update the policy parameters $\theta$ a proportion in the direction of $w_{t+1}$.
\\\\
\parTitle{Critic Update} For the critic, we use a compatible function approximation of the advantage function. The use of a compatible function approximation $A_w(s, a)$ to estimate $A(s, a)$ \citep{sutton2000policy} is a key aspect of the eNAC algorithm. By definition, a compatible function approximation has the property that its gradient can be expressed in terms of the policy. This means, that we can express the advantage function by taking the derivative w.r.t. the policy and multiplying it by $w$:

\begin{align}
\nabla_w A_w(s,a) &= \nabla_{\theta} \log \pi_{\theta}(a|s)\label{eq:adv1}\\
A_w(s,a) &= \nabla_{\theta} \log \pi_{\theta}(a|s)w. \label{eq:adv2}
\end{align}

\noindent To estimate the parameters $w$ of the advantage function, we notice that the discounted sum of advantages can be written in terms of the expected reward and value function

\begin{align}
\sum_{t = 0}^{N}\gamma^t A(s_t, a_t) &= \sum_{t = 0}^{N}\gamma^i r(s_t, a_t) + \gamma^N V(S_{N+1}) - V(S_0),
\end{align}

\noindent where $N$ is the number of steps executed in a trajectory \citep{peters2005natural}. If we assume $\gamma \neq 1$, we can remove the term $\gamma^N V(S_{N+1})$, because in the limit the term becomes zero ($\lim_{N \rightarrow \infty}\gamma^N = 0$). Additionally, if we assume that we always start at the same start state $S_0$, we can write $V(S_0)$ as the cost function $J(\theta)$. Inserting the approximated advantage function (equation \ref{eq:adv2}) for $A(s_t, a_t)$ and bringing the cost function $J(\theta)$ to the left-hand side, yields:

\begin{equation}
	\sum_{i = 0}^{N} \gamma^i \nabla_{\theta} \log \pi_{\theta}(a_i | s_i)^T \cdot w + J(\theta) = \sum_{i = 0}^{N}\gamma^i r(s_i, a_i).
\end{equation}

\noindent This is exactly the equation, which we solve in algorithm \ref{enac-algo} by taking the left pseudoinverse. Besides the parameter vector $w$, we receive the cost function $J(\theta)$ as a side product.
\\\\
\parTitle{Actor Update} The reason why we update the policy parameters in the direction of the critic's parameters is the utilization of a compatible function approximation to estimate the advantage function (equation \ref{eq:adv2}). With this the natural policy gradient from equation \ref{eq:3} simplifies in the following way:

\begin{align}
\nabla_\theta J(\theta) &= \E_{\pi_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(a|s) A_w(s, a)\right]\\
&= \E_{\pi_{\theta}}  \left[\nabla_{\theta} \log \pi_{\theta}(a|s) \nabla_{\theta} \log \pi_{\theta}(a|s)^T w\right]
= G_\theta w\\
\widetilde{\nabla}_{\theta} J(\theta) &= w
\end{align}

\noindent In the next section we present several modifications to the NAC algorithm, which among others improve the accuracy and performance of the critic's estimation, data efficiency, learning stability and matrix inversion.

% ---------------------- Extensions --------------------------------- %
\section{NAC Modifications}
\label{sec:modifications}
\vspace{-2mm}
\parTitle{Least Squares} Besides the episodic NAC the original NAC paper \citep{peters2005natural} featured another approach: NAC using LSTD-Q($\lambda$) \citep{lagoudakis2003least, boyan2002technical}. The main difference is the estimation of the critic's parameters. LSTD-Q($\lambda$) uses least squares temporal difference learning \citep{boyan1999least} to estimate the parameters of the critic after every step taken in the environment. The algorithm uses eligibility traces \cite{sutton2018reinforcement} and two linear functions: $A_w(s,a) = \nabla_{\theta} \log \pi_{\theta}(a|s)^Tw$ and $V_v(s) = \phi(s)v$.  The latter is an approximation of the value function, needed to update the critic by solving a linear set of equations induced by the least squares error between the observation and the value function approximation.
\\\\
\parTitle{Recursive Least Squares} The ``RLS-based natural actor-critic algorithm'' equips the LSTD-Q NAC algorithm with a recursive update rule for the parameters of the critic \cite{park2005rls}. The old parameter values are reused during the current update, which increases efficiency \citep{xu2002efficient}.
\\\\
\parTitle{Fitted NAC \& Importance Sampling} Fitted natural actor-critic (FNAC) is a fitted version of the natural actor-critic algorithm \cite{melo2008fitted}.  It employs a memory $D$, which is filled by sampling data from the environment. Once filled, the least squares NAC algorithm is executed as usual. Normally, after a policy update, we would need to sample $D$ again with the improved policy. However, implementing importance sampling (IS) \cite{sutton2018reinforcement} avoids the re-sampling. In addition to the current policy parameters $\theta$, IS saves the policy parameters $\theta^{-}$, which were used to sample the memory $D$. Hence, the memory $D$ is independent of the current learning policy. Every time we evaluate the critic, we multiply our estimation by the importance weights, \(\tfrac{\pi_{\theta}(a|s)}{\pi_{\theta^{-}}(a|s)}\), to estimate the proportion we need to change the critic's parameters. This approach is extremely data efficient and brings fundamental advantages in situations, where collecting data is costly or time-consuming. Additionally, FNAC makes use of regression methods to update the critic's parameters, which allow the use of a general function approximation for the value function instead of a linear approximation. This positively impacts the accuracy of the critic's estimation. 
\\\\
\parTitle{Incremental NAC (INAC)} The incremental NAC algorithm combines linear function approximation and bootstrapping \citep{bhatnagar2008incremental}. It reuses existing approaches, namely temporal difference learning \citep{sutton2018reinforcement} and two-timescale stochastic approximation \citep{bhatnagar1998two}. Bhatnagar et al. provide three new natural gradient algorithms and prove local convergence. The main feature of the algorithms is the incremental estimation of the policy and the incremental update of the gradient. The policy is changed every time step, and, in comparison to Peters et al., the policy gradient is not reset every update, but saved and reused to calculate the gradient of the next iteration. These improvements facilitate the application to large-scale reinforcement learning problems, decreases computation time and make the algorithm more efficient than conventional actor-critic methods. Further, one of the algorithms can be executed without explicitly computing the inverse Fischer information matrix, which, so the authors, leads to even faster convergence.
\\\\
\parTitle{Implicit Incremental NAC (I2NAC)} INAC algorithms suffer from a difficult to tune step size and an unstable, sometimes divergent, estimation of the natural gradient. The implicit incremental NAC \citep{iwaki2019implicit} uses the ideas of implicit stochastic gradient descent \citep{toulis2014statistical} and implicit temporal differences \citep{tamar2014implicit} to overcome these difficulties. The change between INAC and I2NAC is a scalar weight, which is applied when updating the critic. The algorithm uses eligibility traces and a new hyper-parameter $\gamma$ to compute the weight. This stabilizes learning and empirical results show less divergence.
\\\\
\parTitle{Regularization on NAC} Even if we find the inverse of the Riemannian metric tensor $G$ (equation \ref{eq:ngradient}), it can be ill-defined. An example for this are extremely small eigenvalues, which appear due to noise in the data. These eigenvalues will become extremely large if we take the inverse of $G$ and thus the parameters belonging to the eigenvalues will get a lot of credibility, which they should not have and which falsifies the inverse.
\\
\indent There have been several approaches to introduce a regularization term \cite{sohl2012natural,witsch2011enhancing}. Regularizing the matrix inverse can be done by a technique called ``stochastic robust approximation'' \cite{boyd2004convex}, where $G^{-1}$ is replaced by \(G^{-1}_{\text{reg}} = \left( G^T G + \epsilon I \right)^{-1} G^T\) and $\epsilon$ denotes a small constant (e.g 0.01).
\\
\indent Another idea is the application of ridge regression \cite{hoerl1970ridge}, which has a build in regularizer. We can calculate $\widetilde{\nabla}_{\theta} J(\theta)$ by solving the linear equation \(G(\theta) \widetilde{\nabla}_{\theta} J(\theta) = \nabla_{\theta} J(\theta)\) in the direction of $\widetilde{\nabla}_{\theta} J(\theta)$.

% ---------------------- Discussion --------------------------------- %

\section{Discussion}
\label{sec:discussion}

In this paper, we described the natural gradient, the natural actor-critic algorithm and NAC modifications, which have been applied in the last years. NAC is a model-free state-of-the-art algorithm that can be applied to continuous action spaces. It is parametrization invariant, has been reported to converge faster than vanilla gradient methods and can jump out of plateaus, which is why we expect to see more use of NACs in the future. 

Disadvantages are the inversion of a matrix and the extinction of variance and therefore exploration. For the first, people may need to come up with faster solution to invert matrices or even how to avoid the estimation. The latter has already been tackled by some algorithms, namely TRPO \citep{schulman2015trust} and PPO \citep{schulman2017proximal}. What is more prevailing, however, is a study claiming that NACs exhibit a bias \cite{thomas2014bias}. Further research is needed to shed light on this issue.

Most of the modifications presented in section \ref{sec:modifications} need evaluation on real-world problems to assess their ultimate utility; many of the modifications are only applied to specific environments. Broader research will be necessary to integrate different approaches (eligibility traces, least-squares methods, online implementations) and survey which approach works best in which situations.

In conclusion, NACs work very well for many standard reinforcement learning problems where policy gradient methods are employable. We even encourage the application to POMDPs, where ``The Natural Actor and Belief Critic'' \cite{jurvcivcek2011natural} demonstrated promising first successes.

% -------------------- References ----------------------- %
\newpage
% TODO: alle unten angegbenen Stile machen ganz komische Sachen
% Wer weiß, wie wir das fixen, bitte machen und oben \usepackage{natbib}
% und \bibliographystyle{plain} raus nehmen
% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{NAC-bibliography.bib}   % name your BibTeX data base

\end{document}

