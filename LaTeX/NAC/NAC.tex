%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}

%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}

\usepackage{natbib} % delete before submission
\bibliographystyle{plain} % delete before submission
\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage[noend]{algpseudocode}

%\usepackage{hyperref}

\newcommand{\x}{\item}
\DeclareMathOperator{\E}{\mathbb{E}}
\setlength\parindent{0pt}

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}
\title{Reviewing and improving Natural Actor Critic methods
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Maximilian A. Gehrke\and\\Yannik P. Frisch\and Tabea A. Wilke
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Maximilian A. Gehrke \at
              TU Darmstadt, Germany\\
              \email{maximilian\_alexander.gehrke@stud.tu-darmstadt.de}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           Yannik P. Frisch \at
           TU Darmstadt, Germany\\
           \email{yannik\_phil.frisch@stud.tu-darmstadt.de}
           \and
           Tabea A. Wilke \at
           TU Darmstadt, Germany\\
           \email{tabeaalina.wilke@stud.tu-darmstadt.de}
}
\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor
\maketitle

 % ------------------------------ TASKS -------------------------------------- %
 
% (1) Formality & Language: The report uses the LaTex template and does not exceed 8 pages + 2 pages references. The report is understandable and does not contain any typos, slang or any other errors [10 points]
% (2) Structure & Figures: The report is well structured and has a coherent story that integrates the individual papers known from literature into a bigger picture. Figures and diagrams are well described, labeled, and informative. [10 points]
% (3) Overview: The report provides an extensive overview about the existing literature and provides a summary of the algorithm/platform, variations thereof and its applications [15 points]
% (4) Discussion & Contribution: Besides summarizing, the report compares the existing literature and highlights the differences between approaches. Thereby, introducing a new perspective / overview, which is not present in current literature.

 % ------------------------------- Abstract ---------------------------------- %
\begin{abstract}
In this paper we describe the natural actor critic approach and provide an extensive overview about the current research. This includes a basic description of the natural gradient, actor critic approaches and comparisons between existing extensions. Additionally, we improve the episodic Natural Actor Critic algorithm by applying it with two neural networks instead of basis functions.


% FÜR PROJECT: Our implementation is very basic that can be extended arbitrarily granular. The emphasis of this paper is a detailed description of the implementation process and how it can be used. Results are reported for several of the standard reinforcement learning problems, using the publicly available gym library.

\keywords{Natural Gradient \and Neural Networks \and Advantage Function \and Actor Critic \and NAC}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

% ---------------------- Introduction --------------------------------- %

\newpage
\section{Introduction}
\label{sec:intro}

Policy gradient methods have dominated the field of reinforcement learning in the last years [LOT OF CITES]. These methods represent the policy using differentiable function approximation. They optimize an objective function $J(\theta)$ by repeatedly estimating the gradient of the approximated return w.r.t. the parameters $\theta$ of the policy and updating the parameters in that direction by stochastic gradient descent.

% ----- PRELIMINARIES ----- %

\section{Preliminaries}
\label{sec:preliminaries}

We consider a standard reinforcement learning framework, in which a learning agent interacts with a Markov Decision Process (MDP) \citep{howard1960dynamic, sutton2018reinforcement}. For each discrete time step $t \in \{0,1,2,...\}$, the state, action and reward is denoted as $s_t \in \mathit{S}$, $a_t \in A$ and $r_{t+1} \in \mathit{R} \subset \mathbb{R}$ respectively. The dynamics of the environment are described by the state-transition probabilities $P^a_{ss'} = \text{Pr}\{S_t = s' | S_{t-1} = s, A_{t-1} = a\}$ and the expected immediate rewards $R^a_s = \E[R_t | S_{t-1} = s, A_{t-1} = a]$, for all $s, s' \in S, a \in A$. The agent's behavior at each time step $t$ is specified by a policy $\pi_{\theta}(a|s) = \text{Pr}\{A_t = a | S_{t} = s, \theta\}$, where $\theta$ denotes the parameters of the policy. 
\\\\
We assume that $\pi$ is differentiable w.r.t. it's parameters, so that $\tfrac{\partial\pi(a|s)}{\partial \theta}$ exists and we can estimate the gradient of the objective function $J(\theta)$ by applying the policy gradient theorem \citep{sutton2000policy}

\begin{equation}
\label{equ:policygradienttheorem}
\nabla_{\theta}J(\theta)=\E_{\pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s) Q^{\pi_{\theta}}(s, a)],
\end{equation}

where $Q^{\pi_{\theta}}(s, a)$ denotes an action-value function. One of the most basic policy gradient algorithms, \textit{REINFORCE} \citep{williams1992simple}, estimates the action-value function $Q^{\pi_{\theta}}$ by using the expected discounted return, also known as Monte-Carlo return
	\begin{equation}
	\label{equ:reinforce}
	Q^{\pi_\theta}(s, a) \approx \mathit{G}_t = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1},
	\end{equation}
where $\gamma$ is a discount factor with $\gamma \in [0, 1]$. Policy gradient methods use the gradient of the objective function $J(\theta)$ and a learning rate $\alpha \in \left[0,1\right]$ to recursively update the parameters of $\pi_{\theta}(a|s)$, $\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} J(\theta)$, and find a local optimum. 
\\\\
However, instead of using a Monte-Carlo estimate directly, Actor-Critic methods model the action-value function with a function approximator $Q^{\pi_\theta}(s, a) \approx Q_w(s,a)$ \citep{sutton2000policy}. $Q_w(s,a)$ is called the critic and introduces a second set of parameters, $w$, which need to be optimized; $\pi_{\theta}(a|s)$ is called the actor. By introducing a baseline $B(s,a)$, we can reduce the variance of the action-value function estimate and accelerate learning \citep{sutton2018reinforcement}:

\begin{equation}
\label{equ:policybaseline}
\nabla_{\theta}J(\theta)=\E_{\pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s) Q_w(s, a) - B(s,a)].
\end{equation}

A good baseline with minimal variance is the value function. Subtracting the value function from the action-value function yields the Advantage function $A(s,a) = Q(s,a)-V(s)$. However, the critic can directly estimate the advantage function $A_w(s,a)$ for computing the gradient:

\begin{equation}
	\nabla_{\theta}J(\theta)=\E_{\pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s) A_w(s, a)].
\end{equation}

Optimzing the objective function with vanilla gradient descent is sensitive to parametrization and can be inefficient. One could instead use the \textit{natural gradient}, which is described in the next section.

% ------ NATURAL GRADIENT ----- %

\section{Natural Gradient}
\citep{amari1987differential}
The natural gradient was first introduced by Amari in 1998 \cite{amari1998natural}. The difference between the natural gradient and the ordinary vanilla gradient, is the direction it points to. The ordinary vanilla gradient does only point to the steepest direction, if the parameter space has an Euclidean character [x]. The natural gradient, however, points to the steepest direction of a Riemann parameter space, which is a generalization of the Euclidean parameter space \citep{haykin2009neural, amari1998natural}. This is especially convenient, if our function approximator exhibits a Riemannian character, e.g. neural networks \citep{amari1998natural}.
\\\\
The natural gradient utilizes the $n \times n$ matrix $G = (g_{ij})$, called Riemannian metric tensor, which in general depend on $\theta$. It is defined as
\begin{equation}
	\widetilde{\nabla}_{\theta} J(\theta) = G^{-1} \nabla_\theta J(\theta)
\end{equation}
where $\widetilde{\nabla}_{\theta}$ is the natural gradient w.r.t the parameters $\theta$.  Learning should be carried out with a gradient descent like update rule: $\theta_{t+1} = \theta_{t} + \alpha \widetilde{\nabla}_{\theta} J(\theta)$. In the special case that the parameter space is Euclidean and the coordinate system is orthonormal, the conventional gradient equals the natural gradient: $\widetilde{\nabla}_{\theta} J(\theta) = \nabla_{\theta}$.
\\\\
The natural gradient is the steepest ascent direction of our performance object with respect to any metric. Well, but how do we calculate the matrix $G$? The Hessian $H$ at $x_0$ (which exists since the first derivative vanishes) is the purely covariant form of the Riemann curvature tensor. To estimate the Hessian $H$, we can use the Fisher information metric 
\begin{equation}
	F_\theta=\E_{s\sim\rho^{\pi},a\sim\pi_{\theta}}\left[\nabla_{\theta}log\pi_{\theta}(a|s)^{T}\nabla_{\theta}log\pi_{\theta}(a|s)\right]
\end{equation}
For deterministic policies: 
\begin{equation}
	M_{\mu}(\theta)=E_{s\sim\rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{T}w]
\end{equation}
$\rightarrow$ Limiting case of the Fisher information metric: policy variance reduced to zero.\\
%TODO: Rewrite next part
Combining DPG theorem with compatible function approximation gives 
\begin{equation}
\nabla_{\theta}J(\mu_{\theta}) = E_{s\sim\rho^{\mu}}[\nabla_{\theta} \mu_{\theta}(s) \nabla_{\theta} \mu_{\theta}(s)^{T}w]
\end{equation} 
so steepest ascent direction reduces to
\begin{equation}
M_{\mu}(\theta)^{-1}\nabla_{\theta}J_{\beta}(\mu_{\theta})=w
\end{equation} 
Natural gradient algorithms have been applied successfully in various fields such as road traffic optimization \cite{richter2007natural}, robotic control tasks \cite{kim2010impedance}, motor primitive learning \cite{peters2007applying} and locomotion of a two-linked robot arm \cite{park2005rls}. Using the natural gradient has several advantages and properties, which are listed in the next section.

% ---------------------- Properties --------------------------------- %

\newpage
\section{Properties of the natural gradient}
In the following we present several properties of using the natural gradient for optimizing a reinforcement learning loss function.
\subsection{Online Learning}
The natural gradient can be used online. This means that we can learn from incomplete sequences and by this reduce the variance of our estimation of the return or the advantage function. It is also possible to use an offline, episodic variant of the natural gradient which can be seen in section ??. With both variations at hand, one can make the decision regarding the structure of our problem \cite{pascanu2013revisiting, peters2008natural}.

\subsection{1st order method}
The natural gradient is a first order method, but implements many second order advantages \cite{pascanu2013revisiting}. This is especially relevant for problems, where the cost function is accessible indirectly. For example Deep Boltzmann Machines exhibit this characteristics and recent studies showed possible solutions with natural gradients \cite{desjardins2013metric} whereas second order methods are hard to apply.

\subsection{Parameterization invariant}
The natural gradient is invariant regarding parameterizations. This is a result from the KL-divergence constraint we apply, which measures the changes of our probability density estimation regardless how it was parameterized \cite{pascanu2013revisiting}. This is comparable to signal noise whitening \cite{sohl2012natural}.

\subsection{Faster convergence}
In many cases natural gradient algorithms converge faster than vanilla gradient algorithms \cite{sohl2012natural, amari1998natural}. This is due to the fact that a metric is used which removes the dependencies and differences in scaling between the parameters. This means that these are perpendicular to each other and their space can be explored more efficiently \cite{sohl2012natural}.

\subsection{Better convergence properties}
Using the natural gradient instead of the vanilla gradient avoids getting stuck in local optima during optimization \cite{amari1998natural}.

\subsection{Sample efficiency}
Natural actor critic has much better sample complexity guarantees than gradient-free algorithms, due to the fact that it is a gradient method \cite{nemirovski2005efficient}.

\subsection{More properties}
Look at Berkley slides

\subsection{Drawbacks}
The matrix W is nonsingular (i.e., invertible).

\subsection{NN and learning machines}
1. The algorithm is computationally efficient, since it avoids the need for inverting
the demixing matrix W.
2. The convergence rate of the algorithm is relatively fast.
3. The algorithm lends itself to implementation in the form of an adaptive neural system.
4. Being a stochastic gradient algorithm, the algorithm has a built-in capability to
track statistical variations of a nonstationary environment.
\\
\\
We will present an application of using the natural gradient together with an actor critic method in the next section.

% ---------------------- NAC algorithm --------------------------------- %

\newpage
\section{Natural Actor Critic}
In this section we describe the \textit{Natural Actor Critic} (NAC) algorithm \citep{peters2005natural}. We focus on the trajectory based formulation, called episodic NAC, and present the pseudo code in algorithm \ref{enac-algo}.
\\\\
In episodic NAC, we have a fixed amount of updates $u$ and a fixed amount of steps the agent executes in the environment every update. Therefore, if a trajectory $e$ has reached a terminal state before the agent executed all it's steps, the algorithm samples a new trajectory. This repeats until the maximum number of steps is met and the current trajectory is interrupted. During this process, all states we see, all actions we take and all rewards we get are stored for each trajectory.
\\\\
After sampling, we perform the critic evaluation. We determine the compatible function approximation, the basis functions and the reward statistics for the samples of a single episode and solve a linear equation system to get $w_e$. We update $w_t$ by adding $w_e$ multiplied by a learning rate $\beta \in [0,1]$. We repeat this process for all trajectories encountered during the update. Then, we check if the angle between $w_{t+1}$ and $w$ is smaller than some fixed value $\epsilon$ and if so, we update the policy parameters $\theta$ by adding $w_{t+1}$ multiplied by a learning rate $\alpha \in [0,1]$.

\begin{algorithm}
	\caption{Episodic Natural Actor Critic (eNAC)}\label{enac-algo}
	\begin{algorithmic}[1]
		\REQUIRE Parameterized policy $\pi_{\theta}(a|s)$ and it's derivative $\nabla_\theta\log\pi_{\theta}(a|s)$\\
		\hspace{1.05cm}  with initial parameters $\theta=\theta_0$.
		\FOR{$u = 1,2,3,\dots$}
		\FOR{$e = 1,2,3,\dots$}
		\STATE \textbf{Execute roll-out:} Draw initial state $s_0 \sim p(s_0)$
		\FOR{$t =1,2,3,\dots,N$}
		\STATE Draw action $a_t\sim\pi_{\theta_t}(a_t|s_t)$, observe next state $s_{t+1} \sim p(s_{t+1}|s_t, a_t)$\\
		and reward $r_{t+1} = r(s_t, a_t)$.
		\ENDFOR
		\ENDFOR
		\STATE \textbf{Critic Evaluation (repeat for each sampled trajectory):} Determine compatible function approximation of advantage function $A(s,a) \approx A_{w_t}(s, a)$.
		\STATE Determine basis functions: $\Phi_e = \left[\sum_{t=0}^T\gamma^t\nabla_\theta\log\pi_{\theta}(a_t|s_t)^T, 1\right]^T$, \\
		reward statistics: $R_e=\sum_{t=0}^T\gamma^t r_t$ and solve $\begin{bmatrix} w_{e}\\J \end{bmatrix} = (\Phi_e^T \Phi_e)^{-1} \Phi_e^T R_e$.\\
		Update critic parameters: $w_{t+1} = w_t + \beta w_{e}$.
		
		\STATE \textbf{Actor Update:} When the natural gradient is converged, $\measuredangle (w_{t+1}, w_t)\leq\epsilon$, update the policy parameters: $\theta_{t+1} = \theta_t + \alpha w_{t+1}$.
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

Our cost function is the expected return $\E[G_t]$, which can be written as the discounted sum of advantages, which in return can be written in terms of the expected reward and value function \citep{peters2005natural}

\begin{align}
\sum_{t = 0}^{N}\gamma^t A(s_t, a_t) &= \sum_{t = 0}^{N}\gamma^i r(s_t, a_t) + \gamma^N V(S_{N+1}) - V(S_0),
\end{align}

where $N$ is the number of steps executed in a trajectory. If we assume $\gamma \neq 1$, we can remove the term $\gamma^N V(S_{N+1})$, because in the limit the term becomes zero ($\lim_{N \rightarrow \infty}\gamma^N = 0$). Additionally, if we assume that we always start in the same start state $S_0$, we can write $V(S_0)$ as our cost function $J(\theta)$:

\begin{equation}
	\sum_{t = 0}^{N}\gamma^t A(s_t, a_t) = \sum_{t = 0}^{N}\gamma^t r(s_t, a_t) - J(\theta).
\end{equation}

One of the key aspects of the NAC algorithm is the use of a compatible function approximation $A_w(s, a)$ to estimate $A(s, a)$ \citep{sutton2000policy}, which by definition has the property that it's gradient can be expressed in terms of the policy. This also means, that we can express the advantage function by taking the derivative w.r.t. the policy and multiplying it by $w$:

\begin{align}
	\nabla_w A_w(s,a) &= \nabla_{\theta} \log \pi_{\theta}(s|a)\label{eq:adv1}\\
	A_w(s,a) &= \nabla_{\theta} \log \pi_{\theta}(s|a)w. \label{eq:adv2}
\end{align}

 Inserting this and bringing the cost function $J(\theta)$ to the left hand side:

\begin{equation}
	\sum_{i = 0}^{N} \gamma^i \nabla_{\theta} \log \pi_{\theta}(a_i | s_i)^T \cdot w + 1 \cdot J = \sum_{i = 0}^{N}\gamma^i r(s_i, a_i)
\end{equation}

This is exactly the equation, which we solve in algorithm \ref{enac-algo} by taking the left pseudo inverse to update the critics parameters $w$.
\\\\
The update of the policy parameters in direction of the critic parameters can also be explained by the compatible function approximation (equation \ref{eq:adv2}). With this the natural policy gradient [REF: EQUATION] simplifies:
\begin{align}
\nabla_\theta J(\theta) &= \E_{\pi_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(s|a) A_w(s,a)\right]\\
&= \E_{\pi_{\theta}}  \left[\nabla_{\theta} \log \pi_{\theta}(s|a) \nabla_{\theta} \log \pi_{\theta}(s|a)^T w\right]\\
&= G_\theta w\\
\widetilde{\nabla}_{\theta} J(\theta) &= w
\end{align}

We show several modifications to the algorithm in the next section and discuss it's advantages and drawbacks.

% ---------------------- Extensions --------------------------------- %

\newpage
\section{Modifications}
In this section we list several extensions to the NAC algorithm. 

\subsection{LSTD-Q NAC}

Peters did not only specify the episodic NAC algorithm in his original paper \citep{peters2005natural}, but also an NAC algorithm using LSTD-Q($\lambda$) \citep{lagoudakis2003least, boyan2002technical}. LSTD-Q($\lambda$) uses Least Squares Temporal Difference learning to learn the weighted least squares fixed-point approximation of the state-action value function of a fixed policy $\pi$. one update per action instead of one update per trajectory.


\subsection{Fitted NAC}
Fitted natural actor critic (FNAC) is a fitted version of the natural actor critic algorithm \cite{melo2008fitted}. It allows the utilization of general function approximations and efficient data reuse. This combination is especially appealing, because it combines the faster convergence properties of natural gradient algorithms with the efficient data use of regression algorithms.

\subsection{Importance Sampling}
Melo et. al additionally implemented importance sampling, which increased the efficient use of data even further \cite{melo2008fitted}.

\subsection{Incremental NAC}
``The way we use natural gradients is distinctive in that it is totally incremental: the policy is changed on every time step, yet the gradient computation is never reset as it is in the algorithm of Peters et al. (2005). Alg. 3 is perhaps the most interesting of the three natural-gradient algorithms. It never explicitly stores an estimate of the inverse Fisher information matrix and, as a result, it requires less computation. In empirical experiments using our algorithms (not reported here) we observed that it is easier to ﬁnd good parameter settings for Alg. 3 than it is for the other natural-gradient algorithms and, perhaps because of this, it converged more rapidly than the others and than Konda’s algorithm.'' \cite{bhatnagar2008incremental}

\subsection{Implicit incremental NAC}



\subsection{Regularization on NAC}
	Even if we find the inverse of $G$, it can be ill defined. An example for this are extremely small eigenvalues which appear due to noise in our data. These eigenvalues will become extremely large if we take the inverse of $G$ and thus the parameters belonging to the eigenvalues will get a lot of credibility which they should not have and which will falsify our inverse.
	
	That is why there have been some approaches to introduce a regularization term \cite{sohl2012natural}. Regularizing the matrix inverse can for example be done by a technique called stochastic robust approximation \cite{boyd2004convex}, where $G^{-1}$ is replaced by 
	
	\begin{equation}
		G^{-1}_{\text{reg}} = \left( G^T G + \epsilon I \right)^{-1} G^T
	\end{equation}
	
	\noindent where $\epsilon$ denotes for a small constant (e.g 0.01).
	
	Another idea is the application of ridge regression \cite{hoerl1970ridge}, which has a build in regularizer. We can calculate $\widetilde{\nabla}_{\theta} J(\theta)$ by solving the linear equation
	
	\begin{equation}
		G(\theta) \widetilde{\nabla}_{\theta} J(\theta) = \nabla_{\theta} J(\theta)
	\end{equation}
	
	\noindent in the direction of $\widetilde{\nabla}_{\theta} J(\theta)$.
	
	Witsch et al. use an approach similar to least squares regularization \cite{witsch2011enhancing}. 
	
	\begin{equation}
		\widetilde{\nabla}_{\theta} J(\theta) = \left( F + \lambda I \right)^{-1} \nabla_\theta J(\theta)
	\end{equation}
	
	\noindent If $\lambda$ is huge, the Fisher matrix only has a small influence on the change in direction. Therefore, we want to scale $\lambda$ regarding $F$:
	
	\begin{equation}
		\lambda = \dfrac{\alpha}{\text{det}(F) + 1}
	\end{equation}

	\noindent with $\alpha$ a small constant, e.g. $0.01$.
	
	
	\subsection{POMDPs}
	There have been some approaches to apply the NAC to POMDPS. A promising approach is the Natural Actor and Belief Critic \cite{jurvcivcek2011natural}, which learns parameters in statistical dialogue systems. These can be modeled as POMDPs.
	
	\subsection{Least Squares}
	
	Recursive least squares: \cite{park2005rls}
	
	\subsection{Truncated Natural Policy Gradient}
	
	We use conjugate gradients (CG) to avoid calculating the inverse of the fisher matrix. We apply CG by solving the equation
	
	\begin{equation}
		x_k \approx H_k^{-1} g_k \Rightarrow H^{-1}_k x_k \approx g_k
	\end{equation}
	
	If we transform it into an optimization problem for quadratic equations, we have to optimize
	
	\begin{equation}
		\min_{x \in \mathbb{R}^n} \dfrac{1}{2} x^T H x - g^T x
	\end{equation}

% ---------------------- Discussion --------------------------------- %

\section{Discussion}

Some sources claim that the eNAC and the NAC-LSTD algorithms use a biased estimate of the natural gradient \cite{thomas2014bias}.





% -------------------- References ----------------------- %
\newpage
% TODO: alle unten angegbenen Stile machen ganz komische Sachen
% Wer weiß, wie wir das fixen, bitte machen und oben \usepackage{natbib}
% und \bibliographystyle{plain} raus nehmen
% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{NAC-bibliography.bib}   % name your BibTeX data base

\end{document}

