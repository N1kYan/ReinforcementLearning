%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended, natbib]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Initialize:}}

%TODO Zitierweise Ã¤ndern, sieht schrecklich aus
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Deep Deterministic Policy Gradients: Components and Extensions
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
\subtitle{}

%\titlerunning{Short form of title}        % if too long for running head

\author{Yannik Frisch \and Tabea Wilke \and Maximilian Gehrke %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{F. Author \at
              first address \\
              Tel.: +123-45-678910\\
              Fax: +123-45-678910\\
              \email{fauthor@example.com}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           S. Author \at
              second address
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
TODO
\keywords{DDPG \and DQN \and DPG}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{sec:intro}
%TODO Maybe rewrite this
%TODO Brief introduction about RL and brief description of DPG, DQN and DDPG and their connections
%TODO Purpose of this paper
The field of Reinforcement Learning deals with solving problems that are accessible through the interaction of an agent with it's environment. Such problems can be defined as Markov Decision Processes [x], which consist of a tuple $(S, A, R, P, \gamma)$, where $S$ is the state-distribution, $A$ is the action-distribution, $R:S,A\rightarrow r$ is the reward function mapping states and actions to a scalar reward $r$, $P:S,A\rightarrow S$ is the state transition function mapping states and actions to states, and $\gamma$ is the discount factor used to make an agent more or less farsighted.\\
For many applications the environment details, i.e. $R$ and $P$, are not available. This requires the use of so called model-free algorithms. An early one was Q-Learning [x], which updates an internal representation of the action-value function $Q(s,a)$ by the temporal-difference error [EDIT: EXPLAIN BETTER].\\
The internal representation of this function is not tractable for large state-action spaces. This problem is addressed by value function methods, e.g. the DQN-Algorithm \citep{mnih2013playing}, which is an adaption to Q-Learning, where the action-value function is approximated with deep neural networks.\\
Instead of approximating the value-function, one could also approximate the policy $\pi(s|a)$ directly. 
[EDIT: BRIEFLY DESCRIBE DPG]\\
Actor critic methods \citep{konda2000actor} approximate the value-function as well as the policy.
Finally, the Deep Deterministic Policy Gradient (DDPG) approach \citep{lillicrap2015continuous} combines the above mentioned methods to an actor critic algorithm using neural network function approximation for the policy and the action-value function, which learns a deterministic policy.\\
We will give more detailed insights into the algorithms and how DDPG evolved from them in the next section, before we describe some possible extensions to it in section \ref{sec:1}.

\section{Preliminaries}
The general goal of an reinforcement learning algorithm is to find an optimal behavior policy $\pi(a|s)$, or $\pi(s)$ in the deterministic case, which maximizes the expected total reward an agent collects while following it. An optimal policy can be defined by 
%TODO E richtig darstellen
\[ 
\pi^*(a|s)=\max_a Q^*(s,a)=E \left[
\sum_{t=0}^{T}\gamma^{t}r_t\right] 
\]
Where $t$ is the current time-step and $T$ the final time-step ending an episode.
This policy is just greedily choosing the action maximizing the optimal action-value function $Q^*(s,a)$. By definition this optimal value function yields the bellman equation \citep{sutton2018reinforcement} and can be reinterpreted as maximizing the current reward and the discounted action-value of the resulting state. In formula this gives:
%TODO E
\[
Q^*(s,a) = E_{s'\sim P(s,a), r\sim R(s,a)}\left[
r + \gamma \max_{a'}Q^*(s',a')|s,a \right]
\]
\subsection{Q-Learning}
The Q-Learning approach [x] calculates the optimal value function by ...
\subsection{Deep Q-Learning}
\nocite{mnih2015human}
\label{sec:DQN}
The Deep Q-Network approach (DQN) \citep{mnih2013playing} combines the approximation power of neural networks with the traditional Q-learning. The algorithm is an off-policy, model-free approach and is able to find a close to optimal action-value function for many cases [x] and from this a close to optimal policy. For approximating the action-value function $Q(s,a|\theta)\approx Q(s,a)$ the approach uses a deep neural network with parameters $\theta$, called the Q-Network.
The Q-Network can be trained by sequentially minimizing the loss function $L_i(\theta)$, depending on the parameters
\[
L^{(i)}(\theta)=E_{s,s'\sim\rho^\pi,a\sim\pi^\theta}
\left[\left(r+\gamma \max_{a'} Q(s', 
a'|\theta^{(i-1)})-Q(s,a|\theta^{(i)})\right)^2\right] 
\]
%TODO E richtig machen
This loss function is similar to the classical temporal-difference loss used in Q-Learning, but with approximated action-value functions instead of lookup-tables. Derivating this loss w.r.t. the approximation's weights gives:
%TODO clarify rho and epsilon!!!
\[
\nabla_{\theta^{(i)}}L^{(i)}(\theta)=E_{s,s'\sim\rho^\pi,a\sim\pi^\theta}
\left[\left(r+\gamma \max_{a'} Q(s', 
a'|\theta^{(i-1)})-Q(s,a|\theta^{(i)})\right)\nabla_{\theta^{(i)}}Q(s,a|\theta^{(i)})\right] 
\]
The expectation can be approximated by sampling from an environment and this gradient can be used to optimize the loss function by using stochastic gradient descent.\\
Furthermore, a replay buffer is used which stores samples of the environment. This allows random mini-batch sampling, which decorrelates the samples and is proven to improve the data efficiency [x]. The mini-batch sampling also enables the use of improved derivatives of vanilla stochastic gradient descent, e.g. \textit{RPROP} as in the \textit{Neural Fitted Q-Learning} approach \citep{riedmiller2005neural} or \textit{ADAM Update} \citep{kingma2014adam}.\\
%TODO Rewrite the target network stuff
There are different ways of estimating the expected Q-values. Either with a target network with the same structure as the network for the action-value function or the normal network. If a target network is used, the target weights need to be updated after some training steps \citep{mnih2015human}.\\
A pseudo-code for the DQN approach can be found in algorithm \ref{DQN-algo}.
The DQN approach was able to significantly outperform earlier learning methods despite incorporating almost no prior knowledge about the inputs \citep{mnih2013playing}. However, it is limited by the disability to cope with continuous and high-dimensional action spaces due to the max operator in the action selection \citep{lillicrap2015continuous}. This limitations can be adressed by combining the approach with the Deterministic Policy Gradient, which is described in the following section.

\begin{algorithm}
	\caption{Deep Q-Learning (DQN)}\label{DQN-algo}
	\begin{algorithmic}
		\REQUIRE Replay buffer $\mathit{D}$ with high capacity
		\REQUIRE Neural network for action-value function $\mathit{Q}$
		with random weights $\theta$
		\REQUIRE Neural network for target action-value function$
		\mathit{\hat{Q}}$ with weights $\theta^-=\theta$
		\FOR{episode $1$ \TO $M$}
		\STATE reset environment to state $s_1$
		\FOR{$t=1$ \TO $T$}
		\IF{random $i \le \epsilon$}
		\STATE random action $a_t$
		\ELSE
		\STATE $a_t=\operatorname*{argmax}_a Q(s_t,a|\theta)$
		\ENDIF
		\STATE execute $a_t \rightarrow$ reward $r_t$ and next state 
		$s_{t+1}$
		\STATE save $(s_t, a_t, r_t,s_{t+1})$ in $D$
		\STATE sample mini-batch $(s_i, a_i, r_i,s_{i+1})_k$ of size $k$ from $D$
		\STATE $q_i =
			\begin{cases}
			r_i & \textit{if episode terminates at step i+1}\\
			r_i+\gamma \max_{a'}\hat{Q}(s_{i+1}, a'|\theta^{-})& 
			else\\			
			\end{cases}$
		\STATE perform gradient descent on $\left(q_i-Q\left(s_i, 
		a_i|\theta\right)\right)^2_\theta$
		\STATE every $C$ steps update $\hat{Q}=Q$
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}


\subsection{Deterministic Policy Gradient}
\label{sec:DPG}
Most problems in reinforcement learning consist of a continuous action space which makes it very difficult to greedily choose the best action given a policy, due to the max operator. From a stochastic point of view, the policy is a probability distribution $a\sim\pi(a|s)$ over all actions. In order to calculate the gradient of a parameterized policy $\pi(a,s|\theta)$ over the total reward w.r.t. the weights, one needs to solve an integral over all actions and states, which becomes intractable for large state-action spaces.
From a deterministic view the policy is a discrete mapping from states to actions $a=\pi(s)$ and thus only one integration over the state space is sufficient.\\ 
%TODO Explain likelihood ratio gradients?
The \textit{policy gradient theorem} \citep{sutton2018reinforcement} gives the update rule for a parameterized policy, optimizing the loss function: 
\[
\nabla_{\theta}J(\theta)=E_{s\sim\rho^\pi,a\sim\pi_\theta}
\left[\nabla_\theta \log \pi_\theta(a|s)Q^\pi(s,a)\right] 
\]
From this, a deterministic approach is derived in [EDIT: by?] [\cite{silver2014deterministic}], which gives the update rule for a parameterized deterministic policy function $\pi(s|\theta)$. Rather than trying to maximize the action-value function $Q(s,a)$ globally by greedy improvements of the policy, the authors move the policy in the direction of the gradient of $Q(s,a)$:
\[
\nabla_{\theta^\pi}J\approx E_{s\sim\rho^\pi}\left[\nabla_{\theta^\pi}Q(s,a|\theta^Q)\right]
\]
%TODO s  ... Beta or Pi?
Applying the chain rule to this equation gives the \textit{deterministic policy gradient (DPG) theorem}:
\begin{align*}
\nabla_{\theta^\pi}J\approx E_{s\sim\rho^\pi}\left[\nabla_aQ(s,a|\theta^Q)|_{a=\pi(s|\theta^\pi)} \nabla_{\theta^\pi}\pi(s|\theta^\pi)\right]
\end{align*}
where the expectation can again be approximated by sampling from an environment.
Only using deterministic action outputs will vanish the algorithms exploration, so one needs to make sure there still is exploration [EDIT: FORMULATION]. This is realized by using an off-policy approach which follows a stochastic policy while learning a deterministic policy. The authors also introduce the notion of \textit{compatible function approximation}.
[EDIT: DESCRIBE] Using these to estimate the gradient, an unbiased approximation is guaranteed.\\
The following section describes a typical structure of how to use deep neural networks for function approximation in reinforcement learning. Together with this section this led to the algorithm described in chapter \ref{sec:DDPG}.

\subsection{Actor-Critic Methods}
\label{sec:actor-critic}
A lot of recent success in reinforcement learning is based on \textit{Actor-Critic} methods [\cite{konda2000actor}].
In contrast to value-function or policy-gradient methods, they parameterize both, the value function $Q(s,a) \approx \hat{Q}(s,a|\theta^Q)$, also known as the \textit{Critic}, and the policy $\pi(s|a) \approx  \hat{\pi}(s|a,\theta^\pi)$. To get an intuition about these methods figure \ref{fig:actor-critic} illustrates the update-cycle:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{actor-critic}
	\caption{Intuition about actor-critic methods (figure from 
	\cite{sutton2018reinforcement})}
	\label{fig:actor-critic}
\end{figure}
While the actor learns how to choose the right action and is responsible to update his policy, the critic has to learn and update the parameters of the state-value function. The actors' and the 
critics' parameters can be updated following the TD-error of the critic which is computed from 
the observed reward and the current error of the estimated state values in every time-step. As Fig. 
\ref{fig:actor-critic} illustrates, the actor has no information about the 
current reward and the critic has no direct influence on the actions.  
%TODO direct influence richtig formuliert?
A pseudo-code for an actor-critic method using TD-errors [x] is shown in algorithm \ref{actor-critic-algo}.
\begin{algorithm}
	\caption{Episodic One-step Actor-Critic for Estimating $\pi(a|s,\theta^\pi) \approx \pi^*(a|s)$}\label{actor-critic-algo}
	\begin{algorithmic}
		\REQUIRE Differentiable policy parameterization $\pi(a|s,\theta^\pi)$
		\REQUIRE Differentiable action-value function parameterization $Q(s,a|\theta^Q)$
		\REQUIRE Random initial weights $\theta^\pi$ and $\theta^Q$
		\REQUIRE Step size parameters $\alpha^Q > 0$ and $\alpha^\pi >0$
		\REQUIRE Discount factor $\gamma$
		\FOR{episode $1$ \TO $M$}
		\STATE Get initial state s
		\STATE $i \leftarrow 1$
		\FOR{time-step 1 to T}
		\STATE{Draw action from actor: $a\sim\pi(s|a,\theta^\pi)$}
		\STATE{Do action a, observe reward r and successor state $s'$}
		\STATE{Calculate the TD-error:\\
		\qquad $\delta \leftarrow r + \gamma \max_a' Q(s',a'|\theta^Q) - Q(s,a|\theta^Q)$}
		%TODO Check if TD-error is correct for Q-Function
		\STATE{Update the weights:\\
		\qquad $\theta^Q \leftarrow \theta^Q + \alpha^Q\delta\nabla_{\theta^Q}Q(s,a|\theta^Q)$\\
		\qquad $\theta^\pi \leftarrow \theta^\pi + \alpha^\pi i \delta\nabla_{\theta^\pi}\log\pi(a|s,\theta^\pi)$}
		\STATE{Update:\\
		\qquad $i \leftarrow \gamma i$\\
		\qquad $s \leftarrow s'$}		
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\section{Deep Deterministic Policy Gradient}
\label{sec:DDPG}
\nocite{lillicrap2015continuous}
The combination of above approaches led to the \textit{Deep Deterministic Policy Gradient (DDPG)} approach [\cite{lillicrap2015continuous}], which is a model-free and off-policy algorithm. It can be grouped into the class actor-critic methods and uses a deterministic target policy and deep Q-Learning. Both, the actor and the critic, are realized by deep neural networks. The pseudo-code for DDPG can be found in \ref{DDPG-algo}.\\
It consists of a parameterized deterministic policy, the actor, $\pi(s|\theta^\pi)$ and a parameterized action-value function $Q(s,a|\theta^Q)$, the critic. The critic is updated using the \textit{Bellman Equation} with a TD-error similar in Q-Learning [\cite{watkins1992q}] [EDIT: EQUATION?] and the actor is updated using the DPG theorem [EDIT: LINK? EQUATIONS WITH NUMBERS?].\\
The use of neural networks to parameterize the above functions means that the convergence guarantees do not hold anymore. Therefore the Actor-Critic DPG approach is combined with recent successes from DQN.\\
To ensure independently and identically distributed data, the authors use a replay buffer and sample random mini-batches from it. This again decorrelates the samples and allows the efficient use of hardware optimization, e.g. the ADAM update [\cite{kingma2014adam}].\\
To adress instability issues from applying deep neural network approximation to Q-Learning they also use \textit{target networks} which are copies of the actor $\pi'(s|\theta^{\pi'})$ and the critic $Q'(s,a|\theta^{Q'})$. These target-networks track the learned networks and are constrained to slow changes by using soft updates: $\theta' \leftarrow \tau\theta + (1-\tau)\theta'$ with $\tau << 1$. These consistent targets might slow down the learning process but greatly improve the stability of it.\\
Using low dimensional feature input might give very different scales for the single states. This can lead to problematic learning for the neural networks and is adresses by using \textit{batch normalization} which normalizes each dimension across the samples in a mini-batch.\\
To ensure exploration while using a deterministic policy, a noise process $\mathit{N}$ is added to the action output of the actor network. This noise process can be chosen to suit the environment. 
\begin{algorithm}
	\caption{Deep Deterministic Policy Gradient (DDPG)}\label{DDPG-algo}
	\begin{algorithmic}
		\REQUIRE Replay buffer $\mathit{D}$ with high capacity
		\REQUIRE Critic network $Q(s,a|\theta^Q)$ and actor network $\pi(s|\theta^\pi)$ with random weights $\theta^Q$ and $\theta^\pi$
		\REQUIRE Initialize target networks $Q'$ and $\pi'$ with weights $\theta^{Q'}\leftarrow\theta^Q$ and $\theta^{\pi'}\leftarrow\theta^\pi$
		\FOR{episode $1$ \TO $M$}
		\STATE Initialize random process $\mathit{N}$ for action exploration
		\STATE Reset environment to state $s_1$
		\FOR{$t=1$ \TO $T$}
		\STATE Select action $a_t = \pi(s_t|\theta^\pi) + \mathit{N}_t$ from local actor
		\STATE Execute action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$
		\STATE Save $(s_t, a_t, r_t,s_{t+1})$ in replay buffer $D$
		\STATE Sample mini-batch $(s_i, a_i, r_i,s_{i+1})_k$ with size $k$ from $D$
		\STATE Set TD-target from target networks:\\
		\qquad $y_i = r_i + \gamma Q'(s_{i+1}, \pi'(s_{i+1}|\theta^{\pi'})|\theta^{Q'})$
		\STATE Update the critic by minimizing the loss:\\
		\qquad $L=\frac{1}{N}\sum_i(y_i - Q(s_i,a_i|\theta^Q))^2$
		\STATE Update the actor using the sampled policy gradient:\\ 			\qquad $\nabla_{\theta^\pi}J \approx \frac{1}{N} \sum_i \nabla_a Q(s,a|\theta^Q)|_{s=s_i, a=\pi(s_i)}\nabla_{\theta^\pi}\pi(s|\theta^\pi)|_{s=s_i}$
		\STATE Update the target networks:\\
		\qquad $\theta^{Q'}\leftarrow \tau \theta^Q + (1-\tau)\theta^{Q'}$\\
		\qquad $\theta^{\pi'}\leftarrow \tau \theta^\pi + (1-\tau)\theta^{\pi'}$
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
The algorithm was evaluated on more than 20 simulated physical tasks using the same algorithm, network structures and hyper-parameters, including classic control problems like the cart-pole environment. Using low-dimensional feature input, it was able to find policies performing really well on most of the tasks. Their performance is competitive with these found by a controller with full access to the environment. The algorithm is even able to find good policies using high dimensional pixel input. For simple tasks this turned out to be as fast as using low dimensional state features.\\
The most challenging issues of the approach are his poor sample efficiency and some instabilities. We present some possible extensions to DDPG in the next chapter which might improve on these issues.
\\
\\
\\
TODO:
\begin{itemize}
\item discretizing the action space often suffers from the course of dimensionality
\item naive extension of DPG with nns turns out to be unstable for challenging problems
\end{itemize}


\section{Improvements for DDPG}
Despite it's good performance on many simulated tasks there is still some room to improve the DDPG algorithm. We show some possible extensions for it in this section.
\subsection{Using importance sampling to sample from the replay-buffer}
In practice the algorithm is limited by the maximum storage size $\mathit{N}$ of the replay-buffer $\mathit{D}$. Overwriting older samples by current ones does nowhere differentiate between more or less important experiences, because uniform random samples does weight all experiences equally. One could use a technique similar to \textit{prioritized sweeping} [\cite{moore1993prioritized}] which uses \textit{importance sampling} [\cite{glynn1989importance}] to prefer transitions which are more important over ones that have less value for the training process. 
\subsection{Using Action Noise in Parameter Space}
Instead of adding noise to the action space to ensure exploration, one could add adaptive noise directly to the parameters of the neural network [\cite{plappert2017parameter}]. This would add some randomness into the parameters of the agent and therefore into the decision it makes, while still always fully depending on it's current observation about it's environment. This parameter noise makes an agent's exploration more consistent and results in a more effective exploration, increased performance and smoother behavior. 
\subsection{Evolutionary Approaches}
One can consider an even more extreme case of the above mentioned extension, which would be the use of \textit{Evolutionary Strategies} to approximate the gradient of our objective function [\cite{salimans2017evolution}]. This does not require back-propagation at all and is competitive with sate of the art RL.
\subsection{Improvements of the Deep Neural Network Architectures}
TODO
\label{sec:1}

\section{Conclusion}
\label{sec:conclusion}

%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{ddpg.bib}   % name your BibTeX data base

\end{document}
% end of file template.tex
