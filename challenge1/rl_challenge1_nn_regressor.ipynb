{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.]\n[-1. -1. -8.]\n"
     ]
    }
   ],
   "source": [
    "# Create gym environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()\n",
    "#action space is a Box(1,) with values between [-2,2], joint effort\n",
    "print(env.action_space.low)\n",
    "#observation space is 3d angle of pendulum cos, sin, velocity max:1,1,8; min:-1,-1,8\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17182911 -0.98512677  0.70265143]\n[0.19525401]\n[ 0.17148896 -0.98518604 -0.00690555] -2.004122237986567 False {}\n"
     ]
    }
   ],
   "source": [
    "#reward formular: -(theta^2 + 0.1*theta_dt^2 + 0.001*action^2) (-16.27 is worst, 0 best)\n",
    "print(env.reset())\n",
    "a = env.action_space.sample()\n",
    "print(a)\n",
    "state, reward, done, info = env.step(a)\n",
    "print(state, reward, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model for transition dynamics\n",
    "# Learns regression from state and action to resulting state and reward\n",
    "'''class ControlNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(ControlNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(input_dim, hidden_dim1)\n",
    "        self.middle_linear = torch.nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.output_linear = torch.nn.Linear(hidden_dim2, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_linear(x))\n",
    "        x = F.relu(self.middle_linear(x))\n",
    "        x = F.relu(self.output_linear(x))\n",
    "        return x\n",
    "'''\n",
    "# Neural network model for transition dynamics\n",
    "# Learns regression from state and action to resulting state and reward\n",
    "class ControlNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
    "        super(ControlNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(input_dim, hidden_dim1)\n",
    "        self.input_linear2 = torch.nn.Linear(hidden_dim1, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =self.input_linear(x)\n",
    "        x = self.input_linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size, input dimension, hidden dimension, output dimension\n",
    "# Use  next for next_state + reward as output\n",
    "# n, d_in, d_hid1, d_hid2, d_out = 1, 4, 16, 64, 4 \n",
    "# Use next for next_state as only output\n",
    "n, d_in, d_hid1, d_hid2, d_out = 1, 4, 16, 64, 3\n",
    "# Instance of nn model\n",
    "#model = ControlNet(d_in, d_hid1, d_hid2, d_out).double()\n",
    "model = ControlNet(4,6,3).double()\n",
    "# Criterion for optimisation: Mean Squared Loss\n",
    "criterion = torch.nn.MSELoss(reduction='sum') # reduction sum???\n",
    "# Learning rate for nn\n",
    "l_rate = 0.01\n",
    "# Optimizer: Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=l_rate)\n",
    "# Learning episodes\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy: Random action\n",
    "def policy(s):\n",
    "    a = env.action_space.sample()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... done\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "plot = []\n",
    "state = env.reset()\n",
    "print(\"Training the network...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Render gym environment\n",
    "    # env.render()\n",
    "    # Select action according to policy\n",
    "   \n",
    "    action = policy(state)\n",
    "    # Observations\n",
    "    old_state = state\n",
    "    state, reward, done, info = env.step(action)\n",
    "    # Define tensors holding data\n",
    "    x = Variable(torch.from_numpy(np.array([old_state[0], old_state[1], old_state[2], action])), requires_grad=True)\n",
    "    #y = model.forward(x)\n",
    "    y = model(x)\n",
    "    # Use  next for next_state + reward as output\n",
    "    # true_y = Variable(torch.from_numpy(np.array([state[0], state[1], state[2], reward])), requires_grad=True)\n",
    "    # Use next for next_state only as output\n",
    "    true_y = Variable(torch.from_numpy(np.array([state[0], state[1], state[2]])), requires_grad=True)\n",
    "    # Prints\n",
    "    #print(\"net input tensor: {} with shape {}\".format(net_input,net_input.dim))\n",
    "    \n",
    "    \n",
    "    # Compute loss\n",
    "    #loss = Variable(criterion(true_y, y), requires_grad=True)\n",
    "    loss =criterion(y, true_y)\n",
    "    plot.append(loss)\n",
    "    # Clear gradients, backpropagation and weight update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"... done\")\n",
    "\n",
    "env.close()\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "plt.plot(plot, label=\"Loss per Episode\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
