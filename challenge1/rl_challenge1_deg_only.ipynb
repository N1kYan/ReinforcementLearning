{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.]\n",
      "[-1. -1. -8.]\n"
     ]
    }
   ],
   "source": [
    "# Create gym environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()\n",
    "#action space is a Box(1,) with values between [-2,2], joint effort\n",
    "print(env.action_space.low)\n",
    "#observation space is 3d angle of pendulum cos, sin, velocity max:1,1,8; min:-1,-1,8\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.55400988  0.83251009 -0.75959121]\n",
      "[-1.2791892]\n",
      "[ 0.56755035  0.82333869 -0.32708703] -1.0268486990412888 False {}\n"
     ]
    }
   ],
   "source": [
    "#reward formular: -(theta^2 + 0.1*theta_dt^2 + 0.001*action^2) (-16.27 is worst, 0 best)\n",
    "print(env.reset())\n",
    "a = env.action_space.sample()\n",
    "print(a)\n",
    "state, reward, done, info = env.step(a)\n",
    "print(state, reward, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This method transforms cos and sin input to true degree value by arctan2 function\n",
    "    \n",
    "\"\"\"\n",
    "def my_arctan(cos, sin):\n",
    "    return np.rad2deg(np.arctan2(sin, cos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-180., -160., -140., -120., -100.,  -80.,  -60.,  -40.,  -20.,\n",
      "          0.,   20.,   40.,   60.,   80.,  100.,  120.,  140.,  160.,\n",
      "        180.]), array([-8., -7., -6., -5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,\n",
      "        5.,  6.,  7.,  8.]))\n"
     ]
    }
   ],
   "source": [
    "from Discretization import Discretization\n",
    "\n",
    "larry = Discretization(\"degree_only\", \"Pendulum\",state_space_size=(18+1, 16+1), action_space_size=17)\n",
    "\n",
    "print(larry.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression...\n",
      "25%...\n",
      "50%...\n",
      "75%...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Regression of old state and performed action to new state and observed reward.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Learning episodes / amount of samples for regression\n",
    "epochs = 10000\n",
    "\n",
    "rtx = []\n",
    "rty = []\n",
    "stx = []\n",
    "sty = []\n",
    "plotr = []\n",
    "plots = []\n",
    "\n",
    "regressorReward = RandomForestRegressor(n_estimators=10, min_samples_split=2)\n",
    "regressorState = RandomForestRegressor(n_estimators=10, min_samples_split=2)\n",
    "\n",
    "old_state = env.reset()\n",
    "# Transform from 3 to 2 dim:(cos, sin, angular velocity) -> (angle, angular velocity)\n",
    "old_state = np.array([my_arctan(old_state[0], old_state[1]), old_state[2]])\n",
    "\n",
    "print(\"Regression...\")\n",
    "for i in range(epochs):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    # Transform from 3 to 2 dim:(cos, sin, angular velocity) -> (angle, angular velocity)\n",
    "    next_state = np.array([my_arctan(next_state[0], next_state[1]), next_state[2]])\n",
    "\n",
    "    rtx.append(np.append(old_state ,action))\n",
    "    rty.append(reward)\n",
    "    stx.append(np.append(old_state,action))\n",
    "    sty.append(next_state)\n",
    "    \n",
    "    if i%100==0: # 50 works nicely\n",
    "        \n",
    "        regressorReward.fit(rtx, rty)\n",
    "        fitrtx = regressorReward.predict(rtx)\n",
    "        mse = mean_squared_error(rty, fitrtx)\n",
    "        plotr.append(mse)\n",
    "\n",
    "        \n",
    "        regressorState.fit(stx, sty)\n",
    "        fitstx = regressorState.predict(stx)\n",
    "        mse = mean_squared_error(sty, fitstx)\n",
    "\n",
    "        plots.append(mse)\n",
    "    \n",
    "    old_state = np.copy(next_state)\n",
    "    \n",
    "    if i==int(epochs*0.25):\n",
    "        print(\"25%...\")\n",
    "    if i==int(epochs*0.5):\n",
    "        print(\"50%...\")\n",
    "    if i==int(epochs*0.75):\n",
    "        print(\"75%...\")\n",
    "\n",
    "print(\"...done\")\n",
    "plt.figure(0)\n",
    "plt.plot(plotr, label=\"Loss for reward fitting\")\n",
    "\n",
    "plt.plot(plots, label=\"Loss for state fitting\")\n",
    "plt.legend()\n",
    "print(regressorReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration...\n",
      "Delta:  20.515445464252352\n",
      "Delta:  5.7449635148645\n",
      "Delta:  1.7234890544593497\n",
      "Delta:  0.3289378670903318\n",
      "Delta:  0.09868136012709883\n",
      "Delta:  0.029604408038128582\n",
      "Delta:  0.008881322411440351\n",
      "\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   Value Iteration\n",
    "   \n",
    "   Inputs:\n",
    "   disc: Discriminator object\n",
    "   theta: Minimal value function difference for convergence\n",
    "   gamma: Update learning rate\n",
    "   \n",
    "\"\"\"\n",
    "\n",
    "def value_iteration(disc, theta, gamma):\n",
    "    \n",
    "    print(\"Value iteration...\")\n",
    "\n",
    "    value_function = np.ones(shape=disc.state_space_size)\n",
    "    policy = np.ones(shape=disc.state_space_size)\n",
    "    \n",
    "    delta = theta\n",
    "    \n",
    "    while delta >= theta:\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # Iterate over discrete state space\n",
    "        for s0 in disc.state_space[0]:\n",
    "            for s1 in disc.state_space[1]:\n",
    "                \n",
    "                # Get (only positive) indexes for (possibly negative) discrete state(s)\n",
    "                index = disc.map_to_index([s0, s1])\n",
    "                #print(index)\n",
    "\n",
    "                v = value_function[index[0], index[1]]\n",
    "\n",
    "                # Iterate over all actions to get action maximizing expected reward\n",
    "                amax = 2\n",
    "                rmax = -100\n",
    "                for a in disc.action_space:\n",
    "\n",
    "                    for a in disc.action_space:\n",
    "                        # Get sufficient state and reward from regressors\n",
    "                        x = np.array([s0, s1, a])\n",
    "                        x = x.reshape(1,-1)\n",
    "                        next_s = regressorState.predict(x).T.reshape(-1,)\n",
    "                        r = regressorReward.predict(x)\n",
    "\n",
    "                        # Discretize sufficient state\n",
    "                        next_index = disc.map_to_index([next_s[0], next_s[1]])\n",
    "\n",
    "                        # Calculate expected reward\n",
    "                        # Deterministic case; we do not need probability distribution\n",
    "                        expected_reward = r + gamma*value_function[next_index[0], next_index[1]]\n",
    "\n",
    "                        if rmax < expected_reward:\n",
    "                            amax = a\n",
    "                            rmax = expected_reward \n",
    "\n",
    "                # Define value function by maximum expected reward per state\n",
    "                value_function[index[0], index[1]] = rmax\n",
    "                # Define policy by action achieving maximum expected reward per state\n",
    "                policy[index[0], index[1]] = amax\n",
    "                # Update delta\n",
    "                delta = max(delta, np.abs(v-value_function[index[0], index[1]]))\n",
    "            \n",
    "            if s0 == int((disc.state_space_size[0]-1)*0.25):\n",
    "                print(\"25%...\")\n",
    "            if s0 == int((disc.state_space_size[0]-1)*0.5):\n",
    "                print(\"50%...\")\n",
    "            if s0 == int((disc.state_space_size[0]-1)*0.75):\n",
    "                print(\"75%...\")    \n",
    "\n",
    "        print(\"Delta: \", delta)\n",
    "                    \n",
    "                    \n",
    "    print()\n",
    "    print(\"...done\")\n",
    "    return value_function, policy\n",
    "\n",
    "value_function, policy = value_iteration(disc=larry, theta=0.01, gamma=0.3)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Policy Iteration\n",
    "    \n",
    "    Gives convergence towards the optimal policy by iteratively\n",
    "    performing Policy Evaluation and Policy Improvement\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def policy_iteration(disc, theta, gamma):        \n",
    "    \n",
    "    print(\"Policy iteration...\")\n",
    "\n",
    "    value_function = np.ones(shape=disc.state_space_size)\n",
    "    policy = np.zeros(shape=disc.state_space_size)\n",
    "\n",
    "    \n",
    "    def policy_evaluation(theta, gamma):\n",
    "        print()\n",
    "        print(\"Evaluating policy\")\n",
    "        delta = theta\n",
    "        while delta >= theta:\n",
    "            delta = 0\n",
    "            # Iteratate over discrete state space\n",
    "            for s0 in disc.state_space[0]:\n",
    "                for s1 in disc.state_space[1]:\n",
    "                    for s2 in disc.state_space[2]:\n",
    "                        \n",
    "                        # Get index for state \n",
    "                        # The method already iterates over a discretized state space\n",
    "                        # But the states need to get mapped to a positive index do to possible 'negative' states\n",
    "                        index = disc.map_to_index([s0, s1, s2])\n",
    "                        \n",
    "                        v = value_function[index[0], index[1], index[2]]\n",
    "                        \n",
    "                        \"\"\"\n",
    "                         V(s) = Sum...p(s',r|s,pi(s))[r+gamma*V(s')]\n",
    "                         \n",
    "                        \"\"\"\n",
    "                        a = policy[index[0], index[1], index[2]]\n",
    "                        \n",
    "                        # input for regression\n",
    "                        x = np.array([s0, s1, s2, a]).reshape(1, -1)\n",
    "                        \n",
    "                        # Predict next state and reward with regressors\n",
    "                        next_s = regressorState.predict(x).T.reshape(-1,)\n",
    "                        r = regressorReward.predict(x)      \n",
    "                        \n",
    "                        next_index = disc.map_to_index([next_s[0], next_s[1], next_s[2]])\n",
    "                                          \n",
    "                        value_function[index[0], index[1], index[2]] = r + gamma*value_function[next_index[0],\n",
    "                                                                              next_index[1], next_index[2]]\n",
    "                                          \n",
    "                        delta = max(delta, v - value_function[index[0], index[1], index[2]])\n",
    "            print(\"Delta: \", delta)\n",
    "    \n",
    "    \n",
    "    def policy_improvement(gamma):\n",
    "        print()\n",
    "        print(\"Improving policy\")\n",
    "        policy_stable = True\n",
    "        for s0 in disc.state_space[0]:\n",
    "                for s1 in disc.state_space[1]:\n",
    "                    for s2 in disc.state_space[2]:\n",
    "                        \n",
    "                        # Indexing\n",
    "                        index = disc.map_to_index([s0, s1, s2])\n",
    "                        \n",
    "                        old_action = policy[index[0], index[1], index[2]]\n",
    "                        \n",
    "                        \"\"\"\n",
    "                            pi(s) = argmax_a ... \n",
    "                            We do not have to care about the prob. distribution,\n",
    "                            as we have a deterministic env.\n",
    "                            \n",
    "                        \"\"\"\n",
    "                        # Iterate over all actions and get the one with max. expected reward\n",
    "                        amax = 2\n",
    "                        rmax = -100\n",
    "                        for a in disc.action_space:\n",
    "                            x = np.array([s0, s1, s2, a])\n",
    "                            x = x.reshape(1,-1)\n",
    "                            next_s = regressorState.predict(x).T.reshape(-1,)\n",
    "                            next_index = disc.map_to_index([next_s[0], next_s[1], next_s[2]])\n",
    "                            r = regressorReward.predict(x)\n",
    "                            expected_reward = r + gamma*value_function[next_index[0], next_index[1], next_index[2]]\n",
    "                            if rmax < expected_reward:\n",
    "                                amax = a\n",
    "                                rmax = expected_reward \n",
    "                        policy[index[0], index[1], index[2]] = amax # TODO\n",
    "                        \n",
    "                        if old_action != policy[index[0], index[1], index[2]]:\n",
    "                            policy_stable = False\n",
    "                            \n",
    "        print(\"Policy stable: \", policy_stable)\n",
    "        return policy_stable\n",
    "        \n",
    "    # Run until policy is stable\n",
    "    stable_policy = False\n",
    "    while not stable_policy:\n",
    "        policy_evaluation(theta, gamma)\n",
    "        stable_policy = policy_improvement(gamma)\n",
    "    \n",
    "    print()\n",
    "    print(\"...done\")\n",
    "    return value_function, policy\n",
    "    \n",
    "# value_function, policy = policy_iteration(larry, theta=1, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Episode 1 finished after 200 timesteps\n",
      "Episode 2 finished after 200 timesteps\n",
      "Episode 3 finished after 200 timesteps\n",
      "Episode 4 finished after 200 timesteps\n",
      "Episode 5 finished after 200 timesteps\n",
      "Episode 6 finished after 200 timesteps\n",
      "Episode 7 finished after 200 timesteps\n",
      "Episode 8 finished after 200 timesteps\n",
      "Episode 9 finished after 200 timesteps\n",
      "Episode 10 finished after 200 timesteps\n",
      "Episode 11 finished after 200 timesteps\n",
      "Episode 12 finished after 200 timesteps\n",
      "Episode 13 finished after 200 timesteps\n",
      "Episode 14 finished after 200 timesteps\n",
      "Episode 15 finished after 200 timesteps\n",
      "Episode 16 finished after 200 timesteps\n",
      "Episode 17 finished after 200 timesteps\n",
      "Episode 18 finished after 200 timesteps\n",
      "Episode 19 finished after 200 timesteps\n",
      "Episode 20 finished after 200 timesteps\n",
      "Episode 21 finished after 200 timesteps\n",
      "Episode 22 finished after 200 timesteps\n",
      "Episode 23 finished after 200 timesteps\n",
      "Episode 24 finished after 200 timesteps\n",
      "Episode 25 finished after 200 timesteps\n",
      "Episode 26 finished after 200 timesteps\n",
      "Episode 27 finished after 200 timesteps\n",
      "Episode 28 finished after 200 timesteps\n",
      "Episode 29 finished after 200 timesteps\n",
      "Episode 30 finished after 200 timesteps\n",
      "Episode 31 finished after 200 timesteps\n",
      "Episode 32 finished after 200 timesteps\n",
      "Episode 33 finished after 200 timesteps\n",
      "Episode 34 finished after 200 timesteps\n",
      "Episode 35 finished after 200 timesteps\n",
      "Episode 36 finished after 200 timesteps\n",
      "Episode 37 finished after 200 timesteps\n",
      "Episode 38 finished after 200 timesteps\n",
      "Episode 39 finished after 200 timesteps\n",
      "Episode 40 finished after 200 timesteps\n",
      "Episode 41 finished after 200 timesteps\n",
      "Episode 42 finished after 200 timesteps\n",
      "Episode 43 finished after 200 timesteps\n",
      "Episode 44 finished after 200 timesteps\n",
      "Episode 45 finished after 200 timesteps\n",
      "Episode 46 finished after 200 timesteps\n",
      "Episode 47 finished after 200 timesteps\n",
      "Episode 48 finished after 200 timesteps\n",
      "Episode 49 finished after 200 timesteps\n",
      "Episode 50 finished after 200 timesteps\n",
      "Episode 51 finished after 200 timesteps\n",
      "Episode 52 finished after 200 timesteps\n",
      "Episode 53 finished after 200 timesteps\n",
      "Episode 54 finished after 200 timesteps\n",
      "Episode 55 finished after 200 timesteps\n",
      "Episode 56 finished after 200 timesteps\n",
      "Episode 57 finished after 200 timesteps\n",
      "Episode 58 finished after 200 timesteps\n",
      "Episode 59 finished after 200 timesteps\n",
      "Episode 60 finished after 200 timesteps\n",
      "Episode 61 finished after 200 timesteps\n",
      "Episode 62 finished after 200 timesteps\n",
      "Episode 63 finished after 200 timesteps\n",
      "Episode 64 finished after 200 timesteps\n",
      "Episode 65 finished after 200 timesteps\n",
      "Episode 66 finished after 200 timesteps\n",
      "Episode 67 finished after 200 timesteps\n",
      "Episode 68 finished after 200 timesteps\n",
      "Episode 69 finished after 200 timesteps\n",
      "Episode 70 finished after 200 timesteps\n",
      "Episode 71 finished after 200 timesteps\n",
      "Episode 72 finished after 200 timesteps\n",
      "Episode 73 finished after 200 timesteps\n",
      "Episode 74 finished after 200 timesteps\n",
      "Episode 75 finished after 200 timesteps\n",
      "Episode 76 finished after 200 timesteps\n",
      "Episode 77 finished after 200 timesteps\n",
      "Episode 78 finished after 200 timesteps\n",
      "Episode 79 finished after 200 timesteps\n",
      "Episode 80 finished after 200 timesteps\n",
      "Episode 81 finished after 200 timesteps\n",
      "Episode 82 finished after 200 timesteps\n",
      "Episode 83 finished after 200 timesteps\n",
      "Episode 84 finished after 200 timesteps\n",
      "Episode 85 finished after 200 timesteps\n",
      "Episode 86 finished after 200 timesteps\n",
      "Episode 87 finished after 200 timesteps\n",
      "Episode 88 finished after 200 timesteps\n",
      "Episode 89 finished after 200 timesteps\n",
      "Episode 90 finished after 200 timesteps\n",
      "Episode 91 finished after 200 timesteps\n",
      "Episode 92 finished after 200 timesteps\n",
      "Episode 93 finished after 200 timesteps\n",
      "Episode 94 finished after 200 timesteps\n",
      "Episode 95 finished after 200 timesteps\n",
      "Episode 96 finished after 200 timesteps\n",
      "Episode 97 finished after 200 timesteps\n",
      "Episode 98 finished after 200 timesteps\n",
      "Episode 99 finished after 200 timesteps\n",
      "Episode 100 finished after 200 timesteps\n",
      "...done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Evaluation stuff to see the predictions, discretizations and learned functions in action\n",
    "\n",
    "\"\"\"\n",
    "rewards_per_episode = []\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "for e in range(episodes):\n",
    "    \n",
    "    # Discretize first state\n",
    "    state = env.reset()\n",
    "    state = np.array([my_arctan(state[0], state[1]), state[2]])\n",
    "    index = larry.map_to_index(state)\n",
    "    \n",
    "    cumulative_reward = [0]\n",
    "    \n",
    "    for t in range(200):\n",
    "        # Render environment\n",
    "        # env.render()\n",
    "\n",
    "        # Do step according to policy and get observation and reward\n",
    "        action = np.array([policy[index[0], index[1]]])\n",
    "\n",
    "        #action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        state = np.array([my_arctan(observation[0], observation[1]), observation[2]])\n",
    "        \n",
    "        cumulative_reward.append(cumulative_reward[-1]+reward)\n",
    "        \n",
    "        # Discretize observed state\n",
    "        index = larry.map_to_index(state)\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode {} finished after {} timesteps\".format(e+1, t+1))\n",
    "            break\n",
    "    \n",
    "    rewards_per_episode.append(cumulative_reward)\n",
    "\n",
    "print(\"...done\")    \n",
    "\n",
    "# Average reward over episodes\n",
    "rewards = np.average(rewards_per_episode, axis=0)\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# Plot rewards per timestep averaged over episodes\n",
    "plt.figure()\n",
    "plt.plot(rewards, label='Cumulative reward per timestep, averaged over {} episodes'.format(episodes))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
