{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.]\n",
      "[-1. -1. -8.]\n"
     ]
    }
   ],
   "source": [
    "# Create gym environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()\n",
    "#action space is a Box(1,) with values between [-2,2], joint effort\n",
    "print(env.action_space.low)\n",
    "#observation space is 3d angle of pendulum cos, sin, velocity max:1,1,8; min:-1,-1,8\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16471641 -0.98634097 -0.07883676]\n",
      "[0.19525401]\n",
      "[ 0.1256721  -0.99207183 -0.78930438] -1.9756003490065532 False {}\n"
     ]
    }
   ],
   "source": [
    "#reward formular: -(theta^2 + 0.1*theta_dt^2 + 0.001*action^2) (-16.27 is worst, 0 best)\n",
    "print(env.reset())\n",
    "a = env.action_space.sample()\n",
    "print(a)\n",
    "state, reward, done, info = env.step(a)\n",
    "print(state, reward, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90, 16]\n"
     ]
    }
   ],
   "source": [
    "from Discretization import Discretization\n",
    "import numpy as np\n",
    "\n",
    "larry = Discretization(\"degree_only\", \"Pendulum\",state_space_size=(18+1, 16+1), action_space_size=17)\n",
    "\n",
    "print(larry.map_to_index([np.cos(np.deg2rad(-90)), np.sin(np.deg2rad(-90)), 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression...\n",
      "...done\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Regression of old state and performed action to new state and observed reward.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Learning episodes / amount of samples for regression\n",
    "epochs = 1000\n",
    "\n",
    "rtx = []\n",
    "rty = []\n",
    "stx = []\n",
    "sty = []\n",
    "plotr = []\n",
    "plots = []\n",
    "\n",
    "regressorReward = RandomForestRegressor(n_estimators=10, min_samples_split=2)\n",
    "regressorState = RandomForestRegressor(n_estimators=10, min_samples_split=2)\n",
    "\n",
    "old_state = env.reset()\n",
    "\n",
    "print(\"Regression...\")\n",
    "for i in range(epochs):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    rtx.append(np.append(old_state ,action))\n",
    "    rty.append(reward)\n",
    "    stx.append(np.append(old_state,action))\n",
    "    sty.append(next_state)\n",
    "    \n",
    "    if i%50==0: # 50 works nicely\n",
    "        \n",
    "        regressorReward.fit(rtx, rty)\n",
    "        fitrtx = regressorReward.predict(rtx)\n",
    "        mse = mean_squared_error(rty, fitrtx)\n",
    "        plotr.append(mse)\n",
    "\n",
    "        \n",
    "        regressorState.fit(stx, sty)\n",
    "        fitstx = regressorState.predict(stx)\n",
    "        mse = mean_squared_error(sty, fitstx)\n",
    "\n",
    "        plots.append(mse)\n",
    "    \n",
    "    old_state = np.copy(next_state)\n",
    "\n",
    "print(\"...done\")\n",
    "plt.figure(0)\n",
    "plt.plot(plotr, label=\"Loss for reward fitting\")\n",
    "\n",
    "plt.plot(plots, label=\"Loss for state fitting\")\n",
    "plt.legend()\n",
    "print(regressorReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bb25252e7c5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mvalue_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-bb25252e7c5e>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(disc, theta, gamma)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;31m# Get (only positive) indexes for (possibly negative) discrete state(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ReinforcementLearning/challenge1/Discretization.py\u001b[0m in \u001b[0;36mmap_to_index\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0matan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrad2deg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marctan2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matan\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_space_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   Value Iteration\n",
    "   \n",
    "\"\"\"\n",
    "\n",
    "def value_iteration(disc, theta, gamma):\n",
    "    \n",
    "    print(\"Value iteration...\")\n",
    "\n",
    "    value_function = np.ones(shape=disc.state_space_size)\n",
    "    policy = np.ones(shape=disc.state_space_size)\n",
    "    \n",
    "    delta = theta\n",
    "    \n",
    "    while delta >= theta:\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # Iterate over discrete state space\n",
    "        for s0 in disc.state_space[0]:\n",
    "            for s1 in disc.state_space[1]:\n",
    "                \n",
    "                # Get (only positive) indexes for (possibly negative) discrete state(s)\n",
    "                index = disc.map_to_index([s0, s1])\n",
    "                print(index)\n",
    "\n",
    "                v = value_function[index[0], index[1]]\n",
    "\n",
    "                # Iterate over all actions to get action maximizing expected reward\n",
    "                amax = 2\n",
    "                rmax = -100\n",
    "                for a in disc.action_space:\n",
    "\n",
    "                    for a in disc.action_space:\n",
    "                        # Get sufficient state and reward from regressors\n",
    "                        x = np.array([s0, s1, a])\n",
    "                        x = x.reshape(1,-1)\n",
    "                        next_s = regressorState.predict(x).T.reshape(-1,)\n",
    "                        r = regressorReward.predict(x)\n",
    "\n",
    "                        # Discretize sufficient state\n",
    "                        next_index = disc.map_to_index([next_s[0], next_s[1], next_s[2]])\n",
    "\n",
    "                        # Calculate expected reward\n",
    "                        # Deterministic case; we do not need probability distribution\n",
    "                        expected_reward = r + gamma*value_function[next_index[0], next_index[1]]\n",
    "\n",
    "                        if rmax < expected_reward:\n",
    "                            amax = a\n",
    "                            rmax = expected_reward \n",
    "\n",
    "                # Define value function by maximum expected reward per state\n",
    "                value_function[index[0], index[1]] = rmax\n",
    "                # Define policy by action achieving maximum expected reward per state\n",
    "                policy[index[0], index[1]] = amax\n",
    "\n",
    "        print(\"Delta: \", delta)\n",
    "                    \n",
    "                    \n",
    "    print()\n",
    "    print(\"...done\")\n",
    "    return value_function, policy\n",
    "\n",
    "value_function, policy = value_iteration(disc=larry, theta=1.0, gamma=0.1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Policy Iteration\n",
    "    \n",
    "    Gives convergence towards the optimal policy by iteratively\n",
    "    performing Policy Evaluation and Policy Improvement\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def policy_iteration(disc, theta, gamma):        \n",
    "    \n",
    "    print(\"Policy iteration...\")\n",
    "\n",
    "    value_function = np.ones(shape=disc.state_space_size)\n",
    "    policy = np.zeros(shape=disc.state_space_size)\n",
    "\n",
    "    \n",
    "    def policy_evaluation(theta, gamma):\n",
    "        print()\n",
    "        print(\"Evaluating policy\")\n",
    "        delta = theta\n",
    "        while delta >= theta:\n",
    "            delta = 0\n",
    "            # Iteratate over discrete state space\n",
    "            for s0 in disc.state_space[0]:\n",
    "                for s1 in disc.state_space[1]:\n",
    "                    for s2 in disc.state_space[2]:\n",
    "                        \n",
    "                        # Get index for state \n",
    "                        # The method already iterates over a discretized state space\n",
    "                        # But the states need to get mapped to a positive index do to possible 'negative' states\n",
    "                        index = disc.map_to_index([s0, s1, s2])\n",
    "                        \n",
    "                        v = value_function[index[0], index[1], index[2]]\n",
    "                        \n",
    "                        \"\"\"\n",
    "                         V(s) = Sum...p(s',r|s,pi(s))[r+gamma*V(s')]\n",
    "                         \n",
    "                        \"\"\"\n",
    "                        a = policy[index[0], index[1], index[2]]\n",
    "                        \n",
    "                        # input for regression\n",
    "                        x = np.array([s0, s1, s2, a]).reshape(1, -1)\n",
    "                        \n",
    "                        # Predict next state and reward with regressors\n",
    "                        next_s = regressorState.predict(x).T.reshape(-1,)\n",
    "                        r = regressorReward.predict(x)      \n",
    "                        \n",
    "                        next_index = disc.map_to_index([next_s[0], next_s[1], next_s[2]])\n",
    "                                          \n",
    "                        value_function[index[0], index[1], index[2]] = r + gamma*value_function[next_index[0],\n",
    "                                                                              next_index[1], next_index[2]]\n",
    "                                          \n",
    "                        delta = max(delta, v - value_function[index[0], index[1], index[2]])\n",
    "            print(\"Delta: \", delta)\n",
    "    \n",
    "    \n",
    "    def policy_improvement(gamma):\n",
    "        print()\n",
    "        print(\"Improving policy\")\n",
    "        policy_stable = True\n",
    "        for s0 in disc.state_space[0]:\n",
    "                for s1 in disc.state_space[1]:\n",
    "                    for s2 in disc.state_space[2]:\n",
    "                        \n",
    "                        # Indexing\n",
    "                        index = disc.map_to_index([s0, s1, s2])\n",
    "                        \n",
    "                        old_action = policy[index[0], index[1], index[2]]\n",
    "                        \n",
    "                        \"\"\"\n",
    "                            pi(s) = argmax_a ... \n",
    "                            We do not have to care about the prob. distribution,\n",
    "                            as we have a deterministic env.\n",
    "                            \n",
    "                        \"\"\"\n",
    "                        # Iterate over all actions and get the one with max. expected reward\n",
    "                        amax = 2\n",
    "                        rmax = -100\n",
    "                        for a in disc.action_space:\n",
    "                            x = np.array([s0, s1, s2, a])\n",
    "                            x = x.reshape(1,-1)\n",
    "                            next_s = regressorState.predict(x).T.reshape(-1,)\n",
    "                            next_index = disc.map_to_index([next_s[0], next_s[1], next_s[2]])\n",
    "                            r = regressorReward.predict(x)\n",
    "                            expected_reward = r + gamma*value_function[next_index[0], next_index[1], next_index[2]]\n",
    "                            if rmax < expected_reward:\n",
    "                                amax = a\n",
    "                                rmax = expected_reward \n",
    "                        policy[index[0], index[1], index[2]] = amax # TODO\n",
    "                        \n",
    "                        if old_action != policy[index[0], index[1], index[2]]:\n",
    "                            policy_stable = False\n",
    "                            \n",
    "        print(\"Policy stable: \", policy_stable)\n",
    "        return policy_stable\n",
    "        \n",
    "    # Run until policy is stable\n",
    "    stable_policy = False\n",
    "    while not stable_policy:\n",
    "        policy_evaluation(theta, gamma)\n",
    "        stable_policy = policy_improvement(gamma)\n",
    "    \n",
    "    print()\n",
    "    print(\"...done\")\n",
    "    return value_function, policy\n",
    "    \n",
    "# value_function, policy = policy_iteration(larry, theta=1, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"Current state\", larry.map_to_index(state))\n",
    "    \n",
    "    a = env.action_space.sample()\n",
    "    print(\"Action: \", a)\n",
    "    \n",
    "    # input for regression\n",
    "    x = np.array([state[0], state[1], state[2], a]).reshape(1, -1)\n",
    "    \n",
    "    # Do regression\n",
    "    print(\"Predicted reward: \", regressorReward.predict(x))\n",
    "    state = regressorState.predict(x).T.reshape(-1,)\n",
    "    print(\"Predicted next state: \",  larry.map_to_index(state))                \n",
    "\n",
    "    \n",
    "    # Perform action\n",
    "    state, reward, done, info = env.step(a)\n",
    "    print(\"True reward: \", reward)\n",
    "    print(\"True next state: \", larry.map_to_index(state))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Some stuff to see the predictions, discretizations and learned functions in action\n",
    "\n",
    "\"\"\"\n",
    "rewards_per_episode = []\n",
    "\n",
    "for e in range(5):\n",
    "    \n",
    "    # Discretize first state\n",
    "    index = larry.map_to_index(env.reset())\n",
    "    \n",
    "    rewards_per_timestep = []\n",
    "    \n",
    "    for t in range(1000):\n",
    "        # Render environment\n",
    "        # env.render()\n",
    "\n",
    "        # Do step according to policy and get observation and reward\n",
    "        action = np.array([policy[index[0], index[1], index[2]]])\n",
    "        print(action)\n",
    "        #action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        rewards_per_timestep.append(reward)\n",
    "        \n",
    "        # Discretize observed state\n",
    "        index = larry.map_to_index(observation)\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode {} finished after {} timesteps\".format(e+1, t+1))\n",
    "            break\n",
    "    \n",
    "    rewards_per_episode.append(rewards_per_timestep)\n",
    "\n",
    "# Average reward over episodes\n",
    "rewards = np.average(rewards_per_episode, axis=0)\n",
    "print(np.shape(rewards))\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# Plot rewards per timestep averaged over episodes\n",
    "plt.figure()\n",
    "plt.plot(rewards, label='Average reward per timestep')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
